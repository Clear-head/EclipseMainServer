import asyncio
from playwright.async_api import async_playwright, TimeoutError, Page
import sys, os
from dotenv import load_dotenv

load_dotenv(dotenv_path="src/.env")

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from src.logger.custom_logger import get_logger
from src.service.crawl.utils.store_detail_extractor import StoreDetailExtractor
from src.service.crawl.utils.store_data_saver import StoreDataSaver
from src.service.crawl.utils.crawling_manager import CrawlingManager


class NaverMapContentCrawler:
    """ë„¤ì´ë²„ ì§€ë„ ì½˜í…ì¸ (ë†€ê±°ë¦¬) ê²€ìƒ‰ í¬ë¡¤ë§ í´ë˜ìŠ¤"""
    
    # ì½˜í…ì¸  ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡
    CONTENT_KEYWORDS = [
        "ì„œìš¸ ë™ë¬¼ì¹´í˜",
        "ì„œìš¸ ê³µë°©",
        "ì„œìš¸ ì‚¬ê²©ì¥",
        "ì„œìš¸ ë¯¸ìˆ ê´€",
        "ì„œìš¸ ê·¼êµìœ ì ì§€",
        "ì„œìš¸ ë°•ë¬¼ê´€",
        "ì„œìš¸ í´ë¼ì´ë°",
    ]
    
    def __init__(self, headless: bool = False):
        self.logger = get_logger(__name__)
        self.headless = headless
        self.naver_map_url = "https://map.naver.com/v5/search"
        self.data_saver = StoreDataSaver()
        self.crawling_manager = CrawlingManager("ì½˜í…ì¸ ")
        
        # self.logger.info(f"ë„¤ì´ë²„ ì§€ë„ ì½˜í…ì¸  í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def crawl_by_keywords(
        self, 
        keywords: list = None,
        delay: int = 20
    ):
        """
        í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œ ë„¤ì´ë²„ ì§€ë„ ê²€ìƒ‰ í›„ í¬ë¡¤ë§ (ì œí•œ ì—†ìŒ)
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©)
            delay: í¬ë¡¤ë§ ê°„ ë”œë ˆì´ (ì´ˆ)
        """
        keywords = keywords or self.CONTENT_KEYWORDS
        
        async with async_playwright() as p:
            # ë„¤ì´ë²„ í¬ë¡¤ë§ìš© ë¸Œë¼ìš°ì €
            browser = await p.chromium.launch(
                headless=self.headless,
                args=['--enable-features=ClipboardAPI']
            )
            context = await browser.new_context(
                permissions=['clipboard-read', 'clipboard-write']
            )
            page = await context.new_page()
            
            try:
                # ê° í‚¤ì›Œë“œë³„ë¡œ ê²€ìƒ‰
                for keyword_idx, keyword in enumerate(keywords, 1):
                    # self.logger.info(f"=" * 80)
                    self.logger.info(f"[í‚¤ì›Œë“œ {keyword_idx}/{len(keywords)}] '{keyword}' í¬ë¡¤ë§ ì‹œì‘")
                    # self.logger.info(f"=" * 80)
                    
                    # í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ë° í¬ë¡¤ë§
                    await self._search_and_crawl_all(
                        page, 
                        keyword,
                        delay=delay
                    )
                    
                    self.logger.info(f"[í‚¤ì›Œë“œ {keyword_idx}/{len(keywords)}] '{keyword}' ì™„ë£Œ")
                    
                    # í‚¤ì›Œë“œ ê°„ ëŒ€ê¸°
                    if keyword_idx < len(keywords):
                        await asyncio.sleep(10)
                
                # self.logger.info(f"=" * 80)
                self.logger.info(f"ëª¨ë“  í‚¤ì›Œë“œ í¬ë¡¤ë§ ë° ì €ì¥ ì™„ë£Œ!")
                # self.logger.info(f"=" * 80)
                
            except Exception as e:
                self.logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
            finally:
                await context.close()
                await browser.close()
    
    async def _search_and_crawl_all(
        self, 
        page: Page, 
        keyword: str,
        delay: int = 20
    ):
        """
        ë„¤ì´ë²„ ì§€ë„ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ í›„ ëª©ë¡ í´ë¦­í•˜ì—¬ í¬ë¡¤ë§ ë° ì €ì¥ (ì œí•œ ì—†ìŒ)
        
        Args:
            page: Playwright Page ê°ì²´
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            delay: í¬ë¡¤ë§ ê°„ ë”œë ˆì´
        """
        try:
            # ë„¤ì´ë²„ ì§€ë„ ê²€ìƒ‰
            await page.goto(self.naver_map_url, wait_until='domcontentloaded')
            await asyncio.sleep(3)
            
            # ê²€ìƒ‰ì–´ ì…ë ¥
            search_input_selector = '.input_search'
            await page.wait_for_selector(search_input_selector)
            await page.fill(search_input_selector, '')
            await asyncio.sleep(0.5)
            
            await page.fill(search_input_selector, keyword)
            await page.press(search_input_selector, 'Enter')
            await asyncio.sleep(5)
            
            # ê²€ìƒ‰ ê²°ê³¼ iframe ëŒ€ê¸°
            await page.wait_for_selector('iframe#searchIframe', timeout=10000)
            search_frame_locator = page.frame_locator('iframe#searchIframe')
            search_frame = page.frame('searchIframe')
            await asyncio.sleep(3)
            
            # í˜ì´ì§€ë³„ë¡œ í¬ë¡¤ë§
            page_num = 1
            total_crawled = 0
            
            while True:  # ğŸ”¥ ì œí•œ ì—†ì´ ê³„ì† í¬ë¡¤ë§
                self.logger.info(f"  [{keyword}] {page_num}í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘...")
                
                # í˜„ì¬ í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ëª©ë¡ ë¡œë“œ
                await self._scroll_to_load_all_items(search_frame_locator, search_frame)
                
                # í˜„ì¬ í˜ì´ì§€ì˜ ì•„ì´í…œ ê°œìˆ˜ í™•ì¸
                item_selector = '#_pcmap_list_scroll_container > ul > li'
                items = await search_frame_locator.locator(item_selector).all()
                item_count = len(items)
                
                self.logger.info(f"  [{keyword}] {page_num}í˜ì´ì§€: {item_count}ê°œ ì•„ì´í…œ ë°œê²¬")
                
                if item_count == 0:
                    self.logger.warning(f"  [{keyword}] {page_num}í˜ì´ì§€ì—ì„œ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                # ğŸ”¥ í˜„ì¬ í˜ì´ì§€ì˜ ì•„ì´í…œ ì •ë³´ ìˆ˜ì§‘
                page_items = []
                for idx in range(item_count):
                    items = await search_frame_locator.locator(item_selector).all()
                    
                    if idx >= len(items):
                        break
                    
                    item = items[idx]
                    
                    # ì•„ì´í…œ ì´ë¦„ ì¶”ì¶œ
                    name = await self._extract_item_name(item, idx, item_count)
                    
                    # ì•„ì´í…œ ì •ë³´ ì €ì¥
                    page_items.append({
                        'name': name,
                        'page_num': page_num,
                        'idx': idx,
                        'total_in_page': item_count
                    })
                
                # ğŸ”¥ í˜„ì¬ í˜ì´ì§€ì˜ ì•„ì´í…œë“¤ì„ ë³‘ë ¬ë¡œ í¬ë¡¤ë§
                if page_items:
                    self.logger.info(f"  [{keyword}] {page_num}í˜ì´ì§€ {len(page_items)}ê°œ ì•„ì´í…œ í¬ë¡¤ë§ ì¤‘...")
                    
                    await self.crawling_manager.execute_crawling_with_save(
                        stores=page_items,
                        crawl_func=lambda item, i, t: self._crawl_single_item(
                            page, search_frame_locator, item_selector, item
                        ),
                        save_func=self._save_wrapper,
                        delay=delay
                    )
                    
                    total_crawled += len(page_items)
                
                self.logger.info(f"  [{keyword}] {page_num}í˜ì´ì§€ ì™„ë£Œ (ëˆ„ì  {total_crawled}ê°œ)")
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                has_next = await self._go_to_next_page(search_frame_locator, search_frame)
                
                if not has_next:
                    self.logger.info(f"  [{keyword}] ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬ (ì´ {total_crawled}ê°œ í¬ë¡¤ë§ ì™„ë£Œ)")
                    break
                
                page_num += 1
                await asyncio.sleep(3)
            
        except TimeoutError:
            self.logger.error(f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"'{keyword}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
    
    async def _extract_item_name(self, item, idx: int, item_count: int) -> str:
        """ì•„ì´í…œ ì´ë¦„ ì¶”ì¶œ (4ê°€ì§€ ì„ íƒì ì‹œë„)"""
        name = None
        
        try:
            # 1ì°¨ ì‹œë„: div.Dr2xO > div.pIwpC > a > span.CMy2_
            first_name_selector = 'div.Dr2xO > div.pIwpC > a > span.CMy2_'
            first_name_element = item.locator(first_name_selector).first
            
            if await first_name_element.count() > 0:
                name = await first_name_element.inner_text(timeout=2000)
                name = name.strip()
                # self.logger.debug(f"    [{idx+1}/{item_count}] 1ì°¨ ì„ íƒìë¡œ ì°¾ìŒ: '{name}'")
        except Exception as e:
            self.logger.debug(f"    [{idx+1}/{item_count}] 1ì°¨ ì„ íƒì ì‹¤íŒ¨: {e}")
        
        # 2ì°¨ ì‹œë„
        if not name:
            try:
                name_selector = 'div.qbGlu > div.ouxiq > div.ApCpt > a > span.YwYLL'
                name_element = item.locator(name_selector).first
                
                if await name_element.count() > 0:
                    name = await name_element.inner_text(timeout=2000)
                    name = name.strip()
                    # self.logger.debug(f"    [{idx+1}/{item_count}] 2ì°¨ ì„ íƒìë¡œ ì°¾ìŒ: '{name}'")
            except Exception as e:
                self.logger.debug(f"    [{idx+1}/{item_count}] 2ì°¨ ì„ íƒì ì‹¤íŒ¨: {e}")
        
        # 3ì°¨ ì‹œë„
        if not name:
            try:
                third_name_selector = 'div.Np1CD > div:nth-child(2) > div.SbNoJ > a > span.t3s7S'
                third_name_element = item.locator(third_name_selector).first
                
                if await third_name_element.count() > 0:
                    name = await third_name_element.inner_text(timeout=2000)
                    name = name.strip()
                    # self.logger.debug(f"    [{idx+1}/{item_count}] 3ì°¨ ì„ íƒìë¡œ ì°¾ìŒ: '{name}'")
            except Exception as e:
                self.logger.debug(f"    [{idx+1}/{item_count}] 3ì°¨ ì„ íƒì ì‹¤íŒ¨: {e}")
        
        # 4ì°¨ ì‹œë„: div.Np1CD > div > div.SbNoJ > a > span.t3s7S
        if not name:
            try:
                fourth_name_selector = 'div.Np1CD > div > div.SbNoJ > a > span.t3s7S'
                fourth_name_element = item.locator(fourth_name_selector).first
                
                if await fourth_name_element.count() > 0:
                    name = await fourth_name_element.inner_text(timeout=2000)
                    name = name.strip()
                    # self.logger.debug(f"    [{idx+1}/{item_count}] 4ì°¨ ì„ íƒìë¡œ ì°¾ìŒ: '{name}'")
            except Exception as e:
                self.logger.debug(f"    [{idx+1}/{item_count}] 4ì°¨ ì„ íƒì ì‹¤íŒ¨: {e}")
        
        if not name:
            name = f"ì•„ì´í…œ {idx+1}"
            self.logger.warning(f"    [{idx+1}/{item_count}] ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ ì´ë¦„ ì‚¬ìš©")
        
        return name
    
    async def _crawl_single_item(
        self,
        page: Page,
        search_frame_locator,
        item_selector: str,
        item_info: dict
    ):
        """
        ë‹¨ì¼ ì•„ì´í…œ í¬ë¡¤ë§
        
        Args:
            page: ë©”ì¸ í˜ì´ì§€
            search_frame_locator: ê²€ìƒ‰ frame locator
            item_selector: ì•„ì´í…œ ì„ íƒì
            item_info: ì•„ì´í…œ ì •ë³´ dict
        
        Returns:
            Tuple: (store_data, actual_name) ë˜ëŠ” None
        """
        name = item_info['name']
        idx = item_info['idx']
        
        try:
            # ì•„ì´í…œ ë‹¤ì‹œ ì°¾ê¸°
            items = await search_frame_locator.locator(item_selector).all()
            
            if idx >= len(items):
                self.logger.error(f"'{name}' ì•„ì´í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            item = items[idx]
            
            # í´ë¦­ ìš”ì†Œ ì°¾ê¸°
            click_element = await self._find_click_element(item, idx)
            
            if not click_element:
                self.logger.error(f"'{name}' í´ë¦­ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # í´ë¦­
            try:
                await click_element.scroll_into_view_if_needed()
                await asyncio.sleep(0.3)
                await click_element.click(timeout=5000)
                await asyncio.sleep(3)
            except Exception as click_error:
                self.logger.error(f"'{name}' í´ë¦­ ì‹¤íŒ¨: {click_error}")
                return None
            
            # entryIframe ëŒ€ê¸° ë° í¬ë¡¤ë§
            try:
                await page.wait_for_selector('iframe#entryIframe', timeout=10000)
                entry_frame = page.frame_locator('iframe#entryIframe')
                await asyncio.sleep(3)
                
                # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                extractor = StoreDetailExtractor(entry_frame, page)
                store_data = await extractor.extract_all_details()
                
                if store_data:
                    actual_name = store_data[0]
                    return (store_data, actual_name)
                else:
                    self.logger.error(f"'{name}' ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
                    return None
                
            except TimeoutError:
                self.logger.error(f"'{name}' entryIframeì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None
            except Exception as crawl_error:
                self.logger.error(f"'{name}' í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {crawl_error}")
                return None
                
        except Exception as e:
            self.logger.error(f"'{name}' í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return None
    
    async def _find_click_element(self, item, idx: int):
        """í´ë¦­ ìš”ì†Œ ì°¾ê¸° (4ê°€ì§€ ì„ íƒì ì‹œë„)"""
        # 1ì°¨ ì‹œë„
        try:
            first_link_selector = 'div.Dr2xO > div.pIwpC > a'
            first_element = item.locator(first_link_selector).first
            if await first_element.count() > 0:
                return first_element
        except:
            pass
        
        # 2ì°¨ ì‹œë„
        try:
            click_selector = 'div.qbGlu > div.ouxiq > div.ApCpt > a'
            second_element = item.locator(click_selector).first
            if await second_element.count() > 0:
                return second_element
        except:
            pass
        
        # 3ì°¨ ì‹œë„
        try:
            third_link_selector = 'div.Np1CD > div:nth-child(2) > div.SbNoJ > a'
            third_element = item.locator(third_link_selector).first
            if await third_element.count() > 0:
                return third_element
        except:
            pass
        
        # 4ì°¨ ì‹œë„
        try:
            fourth_link_selector = 'div.Np1CD > div > div.SbNoJ > a'
            fourth_element = item.locator(fourth_link_selector).first
            if await fourth_element.count() > 0:
                return fourth_element
        except:
            pass
        
        # ëª¨ë‘ ì‹¤íŒ¨í•˜ë©´ ì•„ì´í…œ ì „ì²´
        return item
    
    async def _scroll_to_load_all_items(self, search_frame_locator, search_frame):
        """
        ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ì„ ì¡°ê¸ˆì”© ì²œì²œíˆ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ì•„ì´í…œ ë¡œë“œ
        """
        try:
            scroll_container_selector = '#_pcmap_list_scroll_container'
            
            # ìŠ¤í¬ë¡¤ ì»¨í…Œì´ë„ˆ ëŒ€ê¸°
            await search_frame_locator.locator(scroll_container_selector).wait_for(state='visible', timeout=5000)
            
            prev_count = 0
            same_count = 0
            max_same_count = 10
            scroll_step = 500
            
            for scroll_attempt in range(200):
                # í˜„ì¬ ì•„ì´í…œ ê°œìˆ˜
                items = await search_frame_locator.locator(f'{scroll_container_selector} > ul > li').all()
                current_count = len(items)
                
                if current_count == prev_count:
                    same_count += 1
                    if same_count >= max_same_count:
                        # self.logger.info(f"      ìŠ¤í¬ë¡¤ ì™„ë£Œ: ì´ {current_count}ê°œ ì•„ì´í…œ ë¡œë“œ")
                        break
                else:
                    same_count = 0
                    if scroll_attempt % 10 == 0:
                        # self.logger.info(f"      ìŠ¤í¬ë¡¤ ì¤‘... í˜„ì¬ {current_count}ê°œ ë¡œë“œë¨")
                
                prev_count = current_count
                
                try:
                    await search_frame.evaluate(f'''
                        () => {{
                            const container = document.querySelector('{scroll_container_selector}');
                            if (container) {{
                                container.scrollBy({{
                                    top: {scroll_step},
                                    behavior: 'smooth'
                                }});
                            }}
                        }}
                    ''')
                except:
                    pass
                
                await asyncio.sleep(0.5)
            
        except Exception as e:
            self.logger.warning(f"ìŠ¤í¬ë¡¤ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
    
    async def _go_to_next_page(self, search_frame_locator, search_frame) -> bool:
        """ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ (span í…ìŠ¤íŠ¸ê°€ 'ë‹¤ìŒí˜ì´ì§€'ì¸ ë²„íŠ¼ë§Œ ì„ íƒ)"""
        try:
            # ëª¨ë“  í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸°
            next_button_selector = 'a.eUTV2'
            next_buttons = await search_frame_locator.locator(next_button_selector).all()
            
            if len(next_buttons) == 0:
                return False
            
            # "ë‹¤ìŒí˜ì´ì§€" í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ ë²„íŠ¼ ì°¾ê¸°
            for button in next_buttons:
                try:
                    span_text = await button.locator('span').inner_text(timeout=1000)
                    
                    if span_text and 'ë‹¤ìŒí˜ì´ì§€' in span_text:
                        # aria-disabled ì²´í¬
                        is_disabled = await button.get_attribute('aria-disabled')
                        
                        if is_disabled == 'true':
                            return False
                        
                        # ë‹¤ìŒ í˜ì´ì§€ í´ë¦­
                        await button.click()
                        await asyncio.sleep(2)
                        
                        # ìŠ¤í¬ë¡¤ì„ ë§¨ ìœ„ë¡œ ì´ˆê¸°í™”
                        scroll_container_selector = '#_pcmap_list_scroll_container'
                        try:
                            await search_frame.evaluate(f'''
                                () => {{
                                    const container = document.querySelector('{scroll_container_selector}');
                                    if (container) {{
                                        container.scrollTop = 0;
                                    }}
                                }}
                            ''')
                            await asyncio.sleep(1)
                        except:
                            pass
                        
                        # self.logger.info(f"      ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™")
                        return True
                except:
                    continue
            
            # "ë‹¤ìŒí˜ì´ì§€" ë²„íŠ¼ì„ ì°¾ì§€ ëª»í•¨
            return False
                
        except Exception as e:
            self.logger.warning(f"ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    async def _save_wrapper(self, idx: int, total: int, store_data_tuple, store_name: str):
        """
        ì €ì¥ ë˜í¼
        
        Args:
            store_data_tuple: (store_data, actual_name) íŠœí”Œ ë˜ëŠ” None
        """
        if store_data_tuple is None:
            return (False, "í¬ë¡¤ë§ ì‹¤íŒ¨")
        
        store_data, actual_name = store_data_tuple
        
        return await self.data_saver.save_store_data(
            idx=idx,
            total=total,
            store_data=store_data,
            store_name=actual_name,
            log_prefix="ì½˜í…ì¸ "
        )


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger = get_logger(__name__)
    
    headless_mode = False
    crawl_delay = 10
    
    # logger.info("=" * 80)
    logger.info("ë„¤ì´ë²„ ì§€ë„ ì½˜í…ì¸  í¬ë¡¤ë§ ì‹œì‘")
    # logger.info("=" * 80)
    
    try:
        crawler = NaverMapContentCrawler(headless=headless_mode)
        
        await crawler.crawl_by_keywords(
            keywords=None,
            delay=crawl_delay
        )
        
        # logger.info("")
        # logger.info("=" * 80)
        logger.info("ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ!")
        # logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())