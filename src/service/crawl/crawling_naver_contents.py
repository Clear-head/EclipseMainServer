"""
ë„¤ì´ë²„ ì§€ë„ ì½˜í…ì¸ (ë†€ê±°ë¦¬) ê²€ìƒ‰ í¬ë¡¤ë§ ëª¨ë“ˆ (ì´ë¦„ ê¸°ë°˜ ë§¤ì¹­ + ìµœì í™”)
ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì‹œ ìˆœì„œê°€ ë°”ë€Œì–´ë„ ì´ë¦„ìœ¼ë¡œ ì°¾ì•„ì„œ í¬ë¡¤ë§
ê²€ìƒ‰ ìƒíƒœ ìœ ì§€ë¡œ ë¶ˆí•„ìš”í•œ ìŠ¤í¬ë¡¤ ì œê±°
"""
import asyncio
from playwright.async_api import async_playwright, TimeoutError, Page
import sys, os
from dotenv import load_dotenv

load_dotenv(dotenv_path="src/.env")

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from src.logger.custom_logger import get_logger

# ê³µí†µ ëª¨ë“ˆ import
from src.service.crawl.utils.optimized_browser_manager import OptimizedBrowserManager
from src.service.crawl.utils.human_like_actions import HumanLikeActions
from src.service.crawl.utils.scroll_helper import SearchResultScroller, PageNavigator
from src.service.crawl.utils.store_detail_extractor import StoreDetailExtractor
from src.service.crawl.utils.store_data_saver import StoreDataSaver
from src.service.crawl.utils.crawling_manager import CrawlingManager


class NaverMapContentCrawler:
    """ë„¤ì´ë²„ ì§€ë„ ì½˜í…ì¸ (ë†€ê±°ë¦¬) ê²€ìƒ‰ í¬ë¡¤ë§ í´ë˜ìŠ¤ (ì´ë¦„ ê¸°ë°˜ ë§¤ì¹­)"""
    
    CONTENT_KEYWORDS = [
        "ì„œìš¸ ë¯¸ìˆ ê´€",
        "ì„œìš¸ ë™ë¬¼ì¹´í˜",
        "ì„œìš¸ ê³µë°©",
        "ì„œìš¸ ì‚¬ê²©ì¥",
        "ì„œìš¸ ê·¼êµìœ ì ì§€",
        "ì„œìš¸ ë°•ë¬¼ê´€",
        "ì„œìš¸ í´ë¼ì´ë°",
    ]
    
    RESTART_INTERVAL = 30  # 30ê°œë§ˆë‹¤ ì»¨í…ìŠ¤íŠ¸ ì¬ì‹œì‘
    
    def __init__(self, headless: bool = False):
        self.logger = get_logger(__name__)
        self.headless = headless
        self.naver_map_url = "https://map.naver.com/v5/search"
        self.data_saver = StoreDataSaver()
        self.human_actions = HumanLikeActions()
        self.success_count = 0
        self.fail_count = 0
    
    async def crawl_by_keywords(self, keywords: list = None, delay: int = 20):
        """í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œ í¬ë¡¤ë§ (ì´ë¦„ ê¸°ë°˜ ë§¤ì¹­)"""
        keywords = keywords or self.CONTENT_KEYWORDS
        
        async with async_playwright() as p:
            browser = await OptimizedBrowserManager.create_optimized_browser(p, self.headless)
            
            try:
                self.logger.info(f"\n{'='*70}")
                self.logger.info(f"ğŸ“Š ì´ {len(keywords)}ê°œ í‚¤ì›Œë“œ í¬ë¡¤ë§ ì‹œì‘ (ì´ë¦„ ê¸°ë°˜ ë§¤ì¹­)")
                self.logger.info(f"{'='*70}\n")
                
                for keyword_idx, keyword in enumerate(keywords, 1):
                    self.logger.info(f"\n{'='*70}")
                    self.logger.info(f"[í‚¤ì›Œë“œ {keyword_idx}/{len(keywords)}] '{keyword}' í¬ë¡¤ë§ ì‹œì‘")
                    self.logger.info(f"{'='*70}\n")
                    
                    # í‚¤ì›Œë“œë³„ë¡œ í˜ì´ì§€ ë‹¨ìœ„ ì²˜ë¦¬
                    await self._crawl_keyword_by_pages(browser, keyword, delay)
                    
                    self.logger.info(f"âœ… [í‚¤ì›Œë“œ {keyword_idx}/{len(keywords)}] '{keyword}' ì™„ë£Œ\n")
                    
                    if keyword_idx < len(keywords):
                        import random
                        rest_time = random.uniform(40, 60)
                        self.logger.info(f"ğŸ›Œ í‚¤ì›Œë“œ ì™„ë£Œ, {rest_time:.0f}ì´ˆ íœ´ì‹...\n")
                        await asyncio.sleep(rest_time)
                
                self.logger.info(f"\n{'='*70}")
                self.logger.info(f"âœ… ëª¨ë“  í‚¤ì›Œë“œ í¬ë¡¤ë§ ì™„ë£Œ!")
                self.logger.info(f"   ì„±ê³µ: {self.success_count}ê°œ")
                self.logger.info(f"   ì‹¤íŒ¨: {self.fail_count}ê°œ")
                self.logger.info(f"{'='*70}\n")
                
            except Exception as e:
                self.logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
            finally:
                await browser.close()
    
    async def _crawl_keyword_by_pages(self, browser, keyword: str, delay: int):
        """
        í‚¤ì›Œë“œë³„ë¡œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ í¬ë¡¤ë§ (ì´ë¦„ ê¸°ë°˜)
        
        1. ì „ì²´ ì•„ì´í…œì˜ ì´ë¦„ ëª©ë¡ì„ ë¨¼ì € ìˆ˜ì§‘
        2. ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¸Œë¼ìš°ì € ì¬ì‹œì‘
        3. ì´ë¦„ìœ¼ë¡œ ì•„ì´í…œì„ ì°¾ì•„ì„œ í¬ë¡¤ë§
        """
        # âœ… 1ë‹¨ê³„: ì „ì²´ ì•„ì´í…œì˜ ì´ë¦„ ëª©ë¡ ìˆ˜ì§‘
        total_items, total_pages, name_list = await self._get_total_items_with_names(browser, keyword)
        
        if total_items == 0:
            self.logger.warning(f"'{keyword}' ê²°ê³¼ ì—†ìŒ")
            return
        
        self.logger.info(f"âœ… '{keyword}' ì´ {total_items}ê°œ ({total_pages}í˜ì´ì§€)")
        self.logger.info(f"   ğŸ“‹ ìˆ˜ì§‘ëœ ì´ë¦„: {len(name_list)}ê°œ\n")
        
        # âœ… 2ë‹¨ê³„: ë°°ì¹˜ ë‹¨ìœ„ë¡œ í¬ë¡¤ë§
        for batch_start in range(0, total_items, self.RESTART_INTERVAL):
            batch_end = min(batch_start + self.RESTART_INTERVAL, total_items)
            
            batch_num = batch_start // self.RESTART_INTERVAL + 1
            total_batches = (total_items + self.RESTART_INTERVAL - 1) // self.RESTART_INTERVAL
            
            self.logger.info(f"\nğŸ”„ [{keyword}] ë°°ì¹˜ {batch_num}/{total_batches}: {batch_start+1}~{batch_end}/{total_items}")
            
            # ìƒˆ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ë¸Œë¼ìš°ì € ì¬ì‹œì‘)
            context = await OptimizedBrowserManager.create_stealth_context(browser)
            page = await context.new_page()
            
            try:
                # ë°°ì¹˜ ì²˜ë¦¬ (CrawlingManager ì‚¬ìš©)
                await self._process_batch_with_crawling_manager(
                    page, keyword, batch_start, batch_end, name_list, total_items, delay
                )
                
            except Exception as e:
                self.logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
            finally:
                await context.close()
                await asyncio.sleep(3)
                
                if batch_end < total_items:
                    import random
                    rest_time = random.uniform(20, 40)
                    self.logger.info(f"ğŸ›Œ ë°°ì¹˜ ì™„ë£Œ, {rest_time:.0f}ì´ˆ íœ´ì‹...\n")
                    await asyncio.sleep(rest_time)
    
    async def _get_total_items_with_names(self, browser, keyword: str) -> tuple:
        """
        ì „ì²´ ì•„ì´í…œ ê°œìˆ˜, í˜ì´ì§€ ìˆ˜, ì´ë¦„ ëª©ë¡ ìˆ˜ì§‘
        
        Returns:
            Tuple[int, int, List[str]]: (ì „ì²´ ì•„ì´í…œ ìˆ˜, ì „ì²´ í˜ì´ì§€ ìˆ˜, ì´ë¦„ ëª©ë¡)
        """
        context = await browser.new_context()
        page = await context.new_page()
        
        name_list = []
        
        try:
            self.logger.info(f"  ğŸ“‹ '{keyword}' ì „ì²´ ì´ë¦„ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
            
            await page.goto(self.naver_map_url, wait_until='domcontentloaded')
            await asyncio.sleep(3)
            
            # ê²€ìƒ‰
            search_input_selector = '.input_search'
            await page.wait_for_selector(search_input_selector)
            await page.fill(search_input_selector, '')
            await asyncio.sleep(0.5)
            
            await page.fill(search_input_selector, keyword)
            await page.press(search_input_selector, 'Enter')
            await asyncio.sleep(5)
            
            # searchIframe ëŒ€ê¸°
            await page.wait_for_selector('iframe#searchIframe', timeout=10000)
            search_frame_locator = page.frame_locator('iframe#searchIframe')
            search_frame = page.frame('searchIframe')
            
            if not search_frame:
                return 0, 0, []
            
            await asyncio.sleep(3)
            
            # í˜ì´ì§€ë³„ë¡œ ìŠ¤í¬ë¡¤í•˜ì—¬ ì „ì²´ ì´ë¦„ ìˆ˜ì§‘
            total_items = 0
            page_num = 1
            item_selector = '#_pcmap_list_scroll_container > ul > li'
            
            while True:
                # í˜„ì¬ í˜ì´ì§€ ìŠ¤í¬ë¡¤
                await SearchResultScroller.scroll_current_page(
                    search_frame_locator=search_frame_locator,
                    search_frame=search_frame
                )
                
                # í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°
                items = await search_frame_locator.locator(item_selector).all()
                item_count = len(items)
                
                if item_count == 0:
                    break
                
                # ê° ì•„ì´í…œì˜ ì´ë¦„ ì¶”ì¶œ
                for idx, item in enumerate(items):
                    try:
                        name = await self._extract_item_name(item, idx, item_count)
                        name_list.append(name)
                        total_items += 1
                    except Exception as e:
                        self.logger.warning(f"í˜ì´ì§€ {page_num}, ì•„ì´í…œ {idx} ì´ë¦„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        name_list.append(f"ì•„ì´í…œ {total_items + 1}")
                        total_items += 1
                
                # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸
                has_next = await PageNavigator.go_to_next_page_naver(
                    search_frame_locator=search_frame_locator,
                    search_frame=search_frame
                )
                
                if not has_next:
                    break
                
                page_num += 1
                await asyncio.sleep(2)
            
            self.logger.info(f"  âœ… ì´ {total_items}ê°œ ì´ë¦„ ìˆ˜ì§‘ ì™„ë£Œ ({page_num}í˜ì´ì§€)")
            return total_items, page_num, name_list
            
        except Exception as e:
            self.logger.error(f"'{keyword}' ì´ë¦„ ëª©ë¡ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return 0, 0, []
        finally:
            await context.close()
    
    async def _process_batch_with_crawling_manager(
        self,
        page: Page,
        keyword: str,
        batch_start: int,
        batch_end: int,
        name_list: list,
        total: int,
        delay: int
    ):
        """
        ë°°ì¹˜ ë‹¨ìœ„ í¬ë¡¤ë§ (CrawlingManager ì‚¬ìš© + ê²€ìƒ‰ ìƒíƒœ ìœ ì§€)
        
        âœ… í•œ ë²ˆ ê²€ìƒ‰ í›„ ìƒíƒœ ìœ ì§€í•˜ë©° ë§¤ì¥ë³„ë¡œ í¬ë¡¤ë§
        """
        try:
            # âœ… ë„¤ì´ë²„ ì§€ë„ ê²€ìƒ‰ (í•œ ë²ˆë§Œ)
            await page.goto(self.naver_map_url, wait_until='domcontentloaded')
            await asyncio.sleep(3)
            
            search_input_selector = '.input_search'
            await page.wait_for_selector(search_input_selector)
            await page.fill(search_input_selector, '')
            await asyncio.sleep(0.5)
            
            await page.fill(search_input_selector, keyword)
            await page.press(search_input_selector, 'Enter')
            await asyncio.sleep(5)
            
            # searchIframe ëŒ€ê¸°
            await page.wait_for_selector('iframe#searchIframe', timeout=10000)
            search_frame_locator = page.frame_locator('iframe#searchIframe')
            search_frame = page.frame('searchIframe')
            
            if not search_frame:
                self.logger.error("searchIframeì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return 0
            
            await asyncio.sleep(3)
            
            # âœ… ì „ì²´ í˜ì´ì§€ ë¯¸ë¦¬ ë¡œë“œ (í•œ ë²ˆë§Œ)
            await self._load_all_pages(search_frame_locator, search_frame)
            
            item_selector = '#_pcmap_list_scroll_container > ul > li'
            
            # ì´ ë°°ì¹˜ì—ì„œ ì²˜ë¦¬í•  ì´ë¦„ë“¤
            batch_names = name_list[batch_start:batch_end]
            
            # ì´ë¯¸ ì²˜ë¦¬í•œ ì´ë¦„ë“¤ (ì¤‘ë³µ ë°©ì§€)
            processed_names = set()
            
            # ë§¤ì¥ ì •ë³´ ë¦¬ìŠ¤íŠ¸ ìƒì„± (CrawlingManagerìš©)
            stores = []
            for idx, target_name in enumerate(batch_names):
                stores.append({
                    'name': target_name,
                    'global_idx': batch_start + idx
                })
            
            # âœ… CrawlingManagerë¡œ í¬ë¡¤ë§ + ì €ì¥
            crawling_manager = CrawlingManager("ì½˜í…ì¸ ")
            
            await crawling_manager.execute_crawling_with_save(
                stores=stores,
                crawl_func=lambda store, idx, total_stores: self._crawl_single_item_wrapper(
                    page, search_frame_locator, item_selector, store, total, processed_names
                ),
                save_func=lambda idx, total_stores, store_data_tuple, store_name: self._save_wrapper(
                    idx, store_data_tuple, batch_start, total  # âœ… idxì™€ batch_start ì „ë‹¬
                ),
                delay=delay
            )
            
            # ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
            self.success_count += crawling_manager.success_count
            self.fail_count += crawling_manager.fail_count
            
            return batch_end
            
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return 0
    
    async def _load_all_pages(self, search_frame_locator, search_frame):
        """
        ì „ì²´ í˜ì´ì§€ ë¯¸ë¦¬ ë¡œë“œ (í•œ ë²ˆë§Œ)
        
        ëª¨ë“  í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ìŠ¤í¬ë¡¤í•˜ì—¬ ì „ì²´ DOM ë¡œë“œ
        """
        try:
            self.logger.info("  ğŸ“„ ì „ì²´ í˜ì´ì§€ ë¡œë“œ ì¤‘...")
            
            current_page = 1
            
            while True:
                # í˜„ì¬ í˜ì´ì§€ ìŠ¤í¬ë¡¤
                await SearchResultScroller.scroll_current_page(
                    search_frame_locator=search_frame_locator,
                    search_frame=search_frame
                )
                
                # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸
                has_next = await PageNavigator.go_to_next_page_naver(
                    search_frame_locator=search_frame_locator,
                    search_frame=search_frame
                )
                
                if not has_next:
                    break
                
                current_page += 1
                await asyncio.sleep(2)
            
            # âœ… 1í˜ì´ì§€ë¡œ ëŒì•„ê°€ê¸°
            await self._go_to_first_page(search_frame_locator, search_frame)
            
            self.logger.info(f"  âœ… {current_page}í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ\n")
            
        except Exception as e:
            self.logger.warning(f"ì „ì²´ í˜ì´ì§€ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
    
    async def _crawl_single_item_wrapper(
        self,
        page: Page,
        search_frame_locator,
        item_selector: str,
        store: dict,
        total: int,
        processed_names: set
    ):
        """
        CrawlingManagerìš© í¬ë¡¤ë§ ë˜í¼
        
        âœ… ê²€ìƒ‰ ìƒíƒœë¥¼ ìœ ì§€í•˜ë©° ë§¤ì¥ í¬ë¡¤ë§
        """
        target_name = store['name']
        global_idx = store['global_idx']
        
        return await self._crawl_single_item_by_name(
            page=page,
            search_frame_locator=search_frame_locator,
            item_selector=item_selector,
            target_name=target_name,
            global_idx=global_idx,
            total=total,
            processed_names=processed_names
        )
    
    async def _save_wrapper(
        self, 
        idx: int,  # âœ… CrawlingManagerê°€ ì „ë‹¬í•˜ëŠ” ë°°ì¹˜ ë‚´ ì¸ë±ìŠ¤ (1ë¶€í„° ì‹œì‘)
        store_data_tuple, 
        batch_start: int,  # âœ… ë°°ì¹˜ ì‹œì‘ ì¸ë±ìŠ¤
        total: int
    ) -> tuple:
        """CrawlingManagerìš© ì €ì¥ ë˜í¼"""
        if store_data_tuple is None:
            return (False, "í¬ë¡¤ë§ ì‹¤íŒ¨")
        
        store_data, actual_name = store_data_tuple
        
        # âœ… ì „ì²´ ì¸ë±ìŠ¤ ê³„ì‚°: batch_start + idx
        global_idx = batch_start + idx
        
        return await self.data_saver.save_store_data(
            idx=global_idx,  # âœ… ì‹¤ì œ ì¸ë±ìŠ¤ ì „ë‹¬
            total=total,
            store_data=store_data,
            store_name=actual_name,
            log_prefix="ì½˜í…ì¸ "
        )
    
    async def _crawl_single_item_by_name(
        self,
        page: Page,
        search_frame_locator,
        item_selector: str,
        target_name: str,
        global_idx: int,
        total: int,
        processed_names: set
    ):
        """
        ì´ë¦„ìœ¼ë¡œ ì•„ì´í…œì„ ì°¾ì•„ í¬ë¡¤ë§ (ê²€ìƒ‰ ìƒíƒœ ìœ ì§€)
        
        âœ… ë§¤ë²ˆ 1í˜ì´ì§€ë¶€í„° ì°¾ê¸° ì‹œì‘
        """
        try:
            search_frame = page.frame('searchIframe')
            
            if not search_frame:
                self.logger.error("searchIframeì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # âœ… ë§¤ë²ˆ 1í˜ì´ì§€ë¡œ ë¦¬ì…‹
            await self._go_to_first_page(search_frame_locator, search_frame)
            
            current_page = 1
            max_pages = 50
            
            while current_page <= max_pages:
                # í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°
                items = await search_frame_locator.locator(item_selector).all()
                
                # í˜„ì¬ í˜ì´ì§€ì—ì„œ íƒ€ê²Ÿ ì´ë¦„ ì°¾ê¸°
                for idx, current_item in enumerate(items):
                    try:
                        current_name = await self._extract_item_name(current_item, idx, len(items))
                        
                        # íƒ€ê²Ÿ ì´ë¦„ ë°œê²¬ & ì•„ì§ ì²˜ë¦¬ ì•ˆ í–ˆìœ¼ë©´
                        if current_name == target_name and current_name not in processed_names:
                            self.logger.info(f"[{global_idx+1}/{total}] '{target_name}' ë°œê²¬ (í˜ì´ì§€ {current_page})")
                            
                            # í´ë¦­ ìš”ì†Œ ì°¾ê¸°
                            click_element = await self._find_click_element(current_item, idx)
                            
                            if not click_element:
                                self.logger.error(f"[{global_idx+1}/{total}] '{target_name}' í´ë¦­ ìš”ì†Œ ì—†ìŒ")
                                return None
                            
                            # ì‚¬ëŒì²˜ëŸ¼ í´ë¦­
                            await self.human_actions.human_like_click(click_element)
                            await asyncio.sleep(3)
                            
                            # entryIframe ëŒ€ê¸°
                            try:
                                await page.wait_for_selector('iframe#entryIframe', timeout=10000)
                                entry_frame = page.frame_locator('iframe#entryIframe')
                                await asyncio.sleep(3)
                                
                                # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                                extractor = StoreDetailExtractor(entry_frame, page)
                                store_data = await extractor.extract_all_details()
                                
                                if store_data:
                                    actual_name = store_data[0]
                                    
                                    # ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                                    processed_names.add(current_name)
                                    
                                    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
                                    await OptimizedBrowserManager.clear_page_resources(page)
                                    
                                    # âœ… ê²€ìƒ‰ ê²°ê³¼ë¡œ ëŒì•„ê°€ê¸° (ë’¤ë¡œ ê°€ê¸°)
                                    await page.go_back()
                                    await asyncio.sleep(2)
                                    
                                    return (store_data, actual_name)
                                else:
                                    self.logger.error(f"[{global_idx+1}/{total}] '{target_name}' ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
                                    return None
                                    
                            except TimeoutError:
                                self.logger.error(f"[{global_idx+1}/{total}] '{target_name}' entryIframe íƒ€ì„ì•„ì›ƒ")
                                return None
                    
                    except Exception as e:
                        continue
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                has_next = await PageNavigator.go_to_next_page_naver(
                    search_frame_locator=search_frame_locator,
                    search_frame=search_frame
                )
                
                if not has_next:
                    break
                
                current_page += 1
                await asyncio.sleep(2)
            
            # ëê¹Œì§€ ì°¾ì•˜ëŠ”ë° ì—†ìŒ
            self.logger.error(f"[{global_idx+1}/{total}] '{target_name}' ì•„ì´í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None
            
        except Exception as e:
            self.logger.error(f"[{global_idx+1}/{total}] '{target_name}' í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return None
    
    async def _go_to_first_page(self, search_frame_locator, search_frame):
        """í˜ì´ì§€ë„¤ì´ì…˜ì„ 1í˜ì´ì§€ë¡œ ì´ë™"""
        try:
            pagination_selector = 'div.zRM9F > a'
            first_page_button = search_frame_locator.locator(pagination_selector).filter(has_text="1").first
            
            if await first_page_button.count() > 0:
                await first_page_button.click()
                await asyncio.sleep(2)
                self.logger.debug("  ğŸ“„ 1í˜ì´ì§€ë¡œ ì´ë™")
        
        except Exception as e:
            self.logger.debug(f"1í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
    
    async def _extract_item_name(self, item, idx: int, item_count: int) -> str:
        """ì•„ì´í…œ ì´ë¦„ ì¶”ì¶œ (4ê°€ì§€ ì„ íƒì ì‹œë„)"""
        name_selectors = [
            'div.Dr2xO > div.pIwpC > a > span.CMy2_',
            'div.qbGlu > div.ouxiq > div.ApCpt > a > span.YwYLL',
            'div.Np1CD > div:nth-child(2) > div.SbNoJ > a > span.t3s7S',
            'div.Np1CD > div > div.SbNoJ > a > span.t3s7S',
        ]
        
        for selector in name_selectors:
            try:
                name_element = item.locator(selector).first
                if await name_element.count() > 0:
                    name = await name_element.inner_text(timeout=2000)
                    if name and name.strip():
                        return name.strip()
            except:
                continue
        
        return f"ì•„ì´í…œ {idx+1}"
    
    async def _find_click_element(self, item, idx: int):
        """í´ë¦­ ìš”ì†Œ ì°¾ê¸° (4ê°€ì§€ ì„ íƒì ì‹œë„)"""
        link_selectors = [
            'div.Dr2xO > div.pIwpC > a',
            'div.qbGlu > div.ouxiq > div.ApCpt > a',
            'div.Np1CD > div:nth-child(2) > div.SbNoJ > a',
            'div.Np1CD > div > div.SbNoJ > a',
        ]
        
        for selector in link_selectors:
            try:
                element = item.locator(selector).first
                if await element.count() > 0:
                    return element
            except:
                continue
        
        return item


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger = get_logger(__name__)
    
    logger.info("="*70)
    logger.info("ğŸš€ ë„¤ì´ë²„ ì§€ë„ ì½˜í…ì¸  í¬ë¡¤ëŸ¬ ì‹œì‘ (ì´ë¦„ ê¸°ë°˜ ë§¤ì¹­)")
    logger.info("="*70)
    
    try:
        crawler = NaverMapContentCrawler(headless=False)
        
        await crawler.crawl_by_keywords(
            keywords=None,
            delay=15
        )
        
        logger.info("="*70)
        logger.info("ğŸ í¬ë¡¤ëŸ¬ ì¢…ë£Œ")
        logger.info("="*70)
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())