import asyncio
from playwright.async_api import async_playwright, TimeoutError, Page
import sys, os
from dotenv import load_dotenv

load_dotenv(dotenv_path="src/.env")

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from src.logger.custom_logger import get_logger
from src.service.crawl.utils.store_detail_extractor import StoreDetailExtractor
from src.service.crawl.utils.store_data_saver import StoreDataSaver
from src.service.crawl.utils.crawling_manager import CrawlingManager


class NaverMapContentCrawler:
    """ë„¤ì´ë²„ ì§€ë„ ì½˜í…ì¸ (ë†€ê±°ë¦¬) ê²€ìƒ‰ í¬ë¡¤ë§ í´ë˜ìŠ¤"""
    
    # ì½˜í…ì¸  ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡
    CONTENT_KEYWORDS = [
        "ì„œìš¸ ë¯¸ìˆ ê´€",
        "ì„œìš¸ ê·¼êµìœ ì ì§€",
        "ì„œìš¸ ì‚¬ê²©ì¥",
        "ì„œìš¸ ê³µë°©",
        "ì„œìš¸ ë°•ë¬¼ê´€",
        "ì„œìš¸ í´ë¼ì´ë°",
        "ì„œìš¸ ë™ë¬¼ì¹´í˜"
    ]
    
    def __init__(self, headless: bool = False):
        self.logger = get_logger(__name__)
        self.headless = headless
        self.naver_map_url = "https://map.naver.com/v5/search"
        self.data_saver = StoreDataSaver()
        self.crawling_manager = CrawlingManager("ì½˜í…ì¸ ")
        
        self.logger.info(f"âœ“ ë„¤ì´ë²„ ì§€ë„ ì½˜í…ì¸  í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def crawl_by_keywords(
        self, 
        keywords: list = None,
        delay: int = 20
    ):
        """
        í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œ ë„¤ì´ë²„ ì§€ë„ ê²€ìƒ‰ í›„ í¬ë¡¤ë§ (ì œí•œ ì—†ìŒ)
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©)
            delay: í¬ë¡¤ë§ ê°„ ë”œë ˆì´ (ì´ˆ)
        """
        keywords = keywords or self.CONTENT_KEYWORDS
        
        async with async_playwright() as p:
            # ë„¤ì´ë²„ í¬ë¡¤ë§ìš© ë¸Œë¼ìš°ì €
            browser = await p.chromium.launch(
                headless=self.headless,
                args=['--enable-features=ClipboardAPI']
            )
            context = await browser.new_context(
                permissions=['clipboard-read', 'clipboard-write']
            )
            page = await context.new_page()
            
            try:
                # ê° í‚¤ì›Œë“œë³„ë¡œ ê²€ìƒ‰
                for keyword_idx, keyword in enumerate(keywords, 1):
                    self.logger.info(f"=" * 80)
                    self.logger.info(f"[í‚¤ì›Œë“œ {keyword_idx}/{len(keywords)}] '{keyword}' í¬ë¡¤ë§ ì‹œì‘")
                    self.logger.info(f"=" * 80)
                    
                    # í‚¤ì›Œë“œ ì‹œì‘í•  ë•Œë§ˆë‹¤ ì¤‘ë³µ ì²´í¬ ì´ˆê¸°í™” (ë‹¤ë¥¸ í‚¤ì›Œë“œì—ì„œëŠ” ì¤‘ë³µ í—ˆìš©)
                    keyword_crawled_names = set()
                    
                    # í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ë° í¬ë¡¤ë§ (í¬ë¡¤ë§í•˜ë©´ì„œ ë°”ë¡œ ì €ì¥)
                    await self._search_and_crawl_all(
                        page, 
                        keyword,
                        delay=delay,
                        keyword_crawled_names=keyword_crawled_names
                    )
                    
                    self.logger.info(f"[í‚¤ì›Œë“œ {keyword_idx}/{len(keywords)}] '{keyword}' ì™„ë£Œ")
                    
                    # í‚¤ì›Œë“œ ê°„ ëŒ€ê¸°
                    if keyword_idx < len(keywords):
                        await asyncio.sleep(10)
                
                self.logger.info(f"=" * 80)
                self.logger.info(f"ëª¨ë“  í‚¤ì›Œë“œ í¬ë¡¤ë§ ë° ì €ì¥ ì™„ë£Œ!")
                self.logger.info(f"=" * 80)
                
            except Exception as e:
                self.logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
            finally:
                await context.close()
                await browser.close()
    
    async def _search_and_crawl_all(
        self, 
        page: Page, 
        keyword: str,
        delay: int = 20,
        keyword_crawled_names: set = None
    ):
        """
        ë„¤ì´ë²„ ì§€ë„ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ í›„ ëª©ë¡ í´ë¦­í•˜ì—¬ í¬ë¡¤ë§ ë° ì €ì¥ (ì œí•œ ì—†ìŒ)
        
        Args:
            page: Playwright Page ê°ì²´
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            delay: í¬ë¡¤ë§ ê°„ ë”œë ˆì´
            keyword_crawled_names: í˜„ì¬ í‚¤ì›Œë“œì—ì„œ ì´ë¯¸ í¬ë¡¤ë§í•œ ì´ë¦„ë“¤ (ê°™ì€ í‚¤ì›Œë“œ ë‚´ ì¤‘ë³µ ë°©ì§€)
        """
        if keyword_crawled_names is None:
            keyword_crawled_names = set()
            
        try:
            # ë„¤ì´ë²„ ì§€ë„ ê²€ìƒ‰
            await page.goto(self.naver_map_url)
            await asyncio.sleep(2)
            
            # ê²€ìƒ‰ì–´ ì…ë ¥
            search_input_selector = '.input_search'
            await page.wait_for_selector(search_input_selector)
            await page.fill(search_input_selector, '')
            await asyncio.sleep(0.5)
            
            await page.fill(search_input_selector, keyword)
            await page.press(search_input_selector, 'Enter')
            await asyncio.sleep(3)
            
            # ê²€ìƒ‰ ê²°ê³¼ iframe ëŒ€ê¸°
            await page.wait_for_selector('iframe#searchIframe', timeout=10000)
            search_frame_locator = page.frame_locator('iframe#searchIframe')
            search_frame = page.frame('searchIframe')
            await asyncio.sleep(2)
            
            # ëª¨ë“  í˜ì´ì§€ì˜ ê²°ê³¼ í¬ë¡¤ë§ (ì œí•œ ì—†ìŒ)
            crawled_count = 0
            page_num = 1
            
            while True:  # ğŸ”¥ ì œí•œ ì—†ì´ ê³„ì† í¬ë¡¤ë§
                self.logger.info(f"  [{keyword}] {page_num}í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
                
                # í˜„ì¬ í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ëª©ë¡ ë¡œë“œ
                await self._scroll_to_load_all_items(search_frame_locator, search_frame)
                
                # í˜„ì¬ í˜ì´ì§€ì˜ ì•„ì´í…œ ê°œìˆ˜ í™•ì¸
                item_selector = '#_pcmap_list_scroll_container > ul > li'
                items = await search_frame_locator.locator(item_selector).all()
                item_count = len(items)
                
                self.logger.info(f"  [{keyword}] {page_num}í˜ì´ì§€: {item_count}ê°œ ì•„ì´í…œ ë°œê²¬")
                
                if item_count == 0:
                    self.logger.warning(f"  [{keyword}] {page_num}í˜ì´ì§€ì—ì„œ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                # ê° ì•„ì´í…œ í´ë¦­í•˜ì—¬ í¬ë¡¤ë§ ë° ì €ì¥
                for idx in range(item_count):
                    # ë§¤ë²ˆ ëª©ë¡ì„ ë‹¤ì‹œ ê°€ì ¸ì™€ì•¼ í•¨ (DOM ë³€ê²½ ë•Œë¬¸)
                    items = await search_frame_locator.locator(item_selector).all()
                    
                    if idx >= len(items):
                        break
                    
                    item = items[idx]
                    
                    # ğŸ”¥ ì•„ì´í…œ ì´ë¦„ ë° í´ë¦­ ìš”ì†Œ ì°¾ê¸° (ìš°ì„ ìˆœìœ„: div.Dr2xO > div.pIwpC > a)
                    name = None
                    click_element = None
                    
                    try:
                        # 1ì°¨ ì‹œë„: div.Dr2xO > div.pIwpC > a
                        first_selector = 'div.Dr2xO > div.pIwpC > a'
                        first_element = item.locator(first_selector).first
                        
                        if await first_element.count() > 0:
                            name = await first_element.inner_text(timeout=2000)
                            name = name.strip()
                            click_element = first_element
                            self.logger.debug(f"    [{idx+1}/{item_count}] 1ì°¨ ì„ íƒìë¡œ ì°¾ìŒ: '{name}'")
                    except Exception as e:
                        self.logger.debug(f"    [{idx+1}/{item_count}] 1ì°¨ ì„ íƒì ì‹¤íŒ¨: {e}")
                    
                    # 2ì°¨ ì‹œë„: ê¸°ì¡´ ë°©ì‹
                    if not name or not click_element:
                        try:
                            name_selector = 'div.qbGlu > div.ouxiq > div.ApCpt > a > span.YwYLL'
                            name_element = item.locator(name_selector).first
                            
                            if await name_element.count() > 0:
                                name = await name_element.inner_text(timeout=2000)
                                name = name.strip()
                                
                                # í´ë¦­ ìš”ì†ŒëŠ” ë¶€ëª¨ <a> íƒœê·¸
                                click_selector = 'div.qbGlu > div.ouxiq > div.ApCpt > a'
                                click_element = item.locator(click_selector).first
                                self.logger.debug(f"    [{idx+1}/{item_count}] 2ì°¨ ì„ íƒìë¡œ ì°¾ìŒ: '{name}'")
                        except Exception as e:
                            self.logger.debug(f"    [{idx+1}/{item_count}] 2ì°¨ ì„ íƒì ì‹¤íŒ¨: {e}")
                    
                    # ì´ë¦„ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
                    if not name:
                        name = f"ì•„ì´í…œ {idx+1}"
                        self.logger.warning(f"    [{idx+1}/{item_count}] ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ ì´ë¦„ ì‚¬ìš©")
                    
                    # ê°™ì€ í‚¤ì›Œë“œ ë‚´ì—ì„œë§Œ ì¤‘ë³µ ì²´í¬ (ë‹¤ë¥¸ í‚¤ì›Œë“œì—ì„œëŠ” í—ˆìš©)
                    if name in keyword_crawled_names:
                        self.logger.info(f"    [{idx+1}/{item_count}] '{name}' - í˜„ì¬ í‚¤ì›Œë“œì—ì„œ ì´ë¯¸ í¬ë¡¤ë§ë¨, ê±´ë„ˆëœ€")
                        continue
                    
                    self.logger.info(f"    [{idx+1}/{item_count}] '{name}' í¬ë¡¤ë§ ì‹œì‘...")
                    
                    # ì•„ì´í…œ í´ë¦­ (ì°¾ì€ ìš”ì†Œë¡œ í´ë¦­)
                    try:
                        if click_element:
                            # í™”ë©´ì— ë³´ì´ë„ë¡ ìŠ¤í¬ë¡¤
                            await click_element.scroll_into_view_if_needed()
                            await asyncio.sleep(0.3)
                            
                            # í´ë¦­
                            await click_element.click(timeout=5000)
                            await asyncio.sleep(3)
                        else:
                            # í´ë¦­ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì•„ì´í…œ ì „ì²´ í´ë¦­
                            await item.click(timeout=5000)
                            await asyncio.sleep(3)
                        
                    except Exception as click_error:
                        self.logger.error(f"    [{idx+1}/{item_count}] '{name}' í´ë¦­ ì‹¤íŒ¨: {click_error}")
                        continue
                    
                    # entryIframe ëŒ€ê¸° ë° í¬ë¡¤ë§
                    try:
                        await page.wait_for_selector('iframe#entryIframe', timeout=10000)
                        entry_frame = page.frame_locator('iframe#entryIframe')
                        await asyncio.sleep(3)
                        
                        # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                        extractor = StoreDetailExtractor(entry_frame, page)
                        store_data = await extractor.extract_all_details()
                        
                        if store_data:
                            actual_name = store_data[0]  # ì¶”ì¶œëœ ì‹¤ì œ ì´ë¦„
                            keyword_crawled_names.add(actual_name)
                            crawled_count += 1
                            self.logger.info(f"    [{idx+1}/{item_count}] '{actual_name}' í¬ë¡¤ë§ ì™„ë£Œ âœ“")
                            
                            # ğŸ”¥ í¬ë¡¤ë§ ì§í›„ ë°”ë¡œ ì €ì¥
                            try:
                                self.logger.info(f"    [{idx+1}/{item_count}] '{actual_name}' DB ì €ì¥ ì‹œì‘...")
                                result = await self._save_wrapper(crawled_count, item_count, store_data, actual_name)
                                
                                if result:
                                    success, msg = result
                                    if success:
                                        self.logger.info(f"    [{idx+1}/{item_count}] '{actual_name}' DB ì €ì¥ ì™„ë£Œ âœ“âœ“")
                                    else:
                                        self.logger.error(f"    [{idx+1}/{item_count}] '{actual_name}' DB ì €ì¥ ì‹¤íŒ¨: {msg}")
                                else:
                                    self.logger.error(f"    [{idx+1}/{item_count}] '{actual_name}' DB ì €ì¥ ê²°ê³¼ ì—†ìŒ")
                                    
                            except Exception as save_error:
                                self.logger.error(f"    [{idx+1}/{item_count}] '{actual_name}' DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {save_error}")
                                import traceback
                                self.logger.error(traceback.format_exc())
                        else:
                            self.logger.error(f"    [{idx+1}/{item_count}] '{name}' ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
                        
                    except TimeoutError:
                        self.logger.error(f"    [{idx+1}/{item_count}] '{name}' entryIframeì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    except Exception as crawl_error:
                        self.logger.error(f"    [{idx+1}/{item_count}] '{name}' í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {crawl_error}")
                    
                    # ë”œë ˆì´
                    if idx < item_count - 1:
                        await asyncio.sleep(delay)
                
                self.logger.info(f"  [{keyword}] {page_num}í˜ì´ì§€ ì™„ë£Œ (ëˆ„ì  {crawled_count}ê°œ)")
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                has_next = await self._go_to_next_page(search_frame_locator)
                
                if not has_next:
                    self.logger.info(f"  [{keyword}] ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬ (ì´ {crawled_count}ê°œ í¬ë¡¤ë§ ì™„ë£Œ)")
                    break
                
                page_num += 1
                await asyncio.sleep(3)
            
        except TimeoutError:
            self.logger.error(f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"'{keyword}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
    
    async def _scroll_to_load_all_items(self, search_frame_locator, search_frame):
        """
        ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ì„ ì¡°ê¸ˆì”© ì²œì²œíˆ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ì•„ì´í…œ ë¡œë“œ
        """
        try:
            scroll_container_selector = '#_pcmap_list_scroll_container'
            
            # ìŠ¤í¬ë¡¤ ì»¨í…Œì´ë„ˆ ëŒ€ê¸°
            await search_frame_locator.locator(scroll_container_selector).wait_for(state='visible', timeout=5000)
            
            prev_count = 0
            same_count = 0
            max_same_count = 10
            scroll_step = 500
            
            for scroll_attempt in range(200):
                # í˜„ì¬ ì•„ì´í…œ ê°œìˆ˜
                items = await search_frame_locator.locator(f'{scroll_container_selector} > ul > li').all()
                current_count = len(items)
                
                if current_count == prev_count:
                    same_count += 1
                    if same_count >= max_same_count:
                        self.logger.info(f"      ìŠ¤í¬ë¡¤ ì™„ë£Œ: ì´ {current_count}ê°œ ì•„ì´í…œ ë¡œë“œ")
                        break
                else:
                    same_count = 0
                    if scroll_attempt % 10 == 0:
                        self.logger.info(f"      ìŠ¤í¬ë¡¤ ì¤‘... í˜„ì¬ {current_count}ê°œ ë¡œë“œë¨")
                
                prev_count = current_count
                
                try:
                    await search_frame.evaluate(f'''
                        () => {{
                            const container = document.querySelector('{scroll_container_selector}');
                            if (container) {{
                                container.scrollBy({{
                                    top: {scroll_step},
                                    behavior: 'smooth'
                                }});
                            }}
                        }}
                    ''')
                except:
                    pass
                
                await asyncio.sleep(0.5)
            
        except Exception as e:
            self.logger.warning(f"ìŠ¤í¬ë¡¤ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
    
    async def _go_to_next_page(self, search_frame_locator) -> bool:
        """ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™"""
        try:
            next_button_selector = 'a.eUTV2'
            next_button = search_frame_locator.locator(next_button_selector)
            
            if await next_button.count() > 0:
                is_disabled = await next_button.get_attribute('aria-disabled')
                
                if is_disabled == 'true':
                    return False
                
                await next_button.click()
                await asyncio.sleep(2)
                
                self.logger.info(f"      ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™")
                return True
            else:
                return False
                
        except Exception as e:
            self.logger.warning(f"ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    async def _save_wrapper(self, idx: int, total: int, store_data: tuple, store_name: str):
        """ì €ì¥ ë˜í¼"""
        return await self.data_saver.save_store_data(
            idx=idx,
            total=total,
            store_data=store_data,
            store_name=store_name,
            log_prefix="ì½˜í…ì¸ "
        )


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger = get_logger(__name__)
    
    headless_mode = False
    crawl_delay = 20
    
    logger.info("=" * 80)
    logger.info("ë„¤ì´ë²„ ì§€ë„ ì½˜í…ì¸  í¬ë¡¤ë§ ì‹œì‘")
    logger.info("=" * 80)
    
    try:
        crawler = NaverMapContentCrawler(headless=headless_mode)
        
        await crawler.crawl_by_keywords(
            keywords=None,
            delay=crawl_delay
        )
        
        logger.info("")
        logger.info("=" * 80)
        logger.info("âœ“ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())


if __name__ == '__main__':
    asyncio.run(main())