"""
ë„¤ì´ë²„ ì§€ë„ ì½˜í…ì¸ (ë†€ê±°ë¦¬) ê²€ìƒ‰ í¬ë¡¤ë§ ëª¨ë“ˆ (ë©”ëª¨ë¦¬ ìµœì í™” + ë´‡ ìš°íšŒ + ë³‘ë ¬ ì²˜ë¦¬)
ëª©ë¡ í´ë¦­ ë°©ì‹
"""
import asyncio
from playwright.async_api import async_playwright, TimeoutError, Page
import sys, os
from dotenv import load_dotenv

load_dotenv(dotenv_path="src/.env")

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from src.logger.custom_logger import get_logger

# ê³µí†µ ëª¨ë“ˆ import
from src.service.crawl.utils.optimized_browser_manager import OptimizedBrowserManager
from src.service.crawl.utils.human_like_actions import HumanLikeActions
from src.service.crawl.utils.scroll_helper import SearchResultScroller, PageNavigator
from src.service.crawl.utils.store_detail_extractor import StoreDetailExtractor
from src.service.crawl.utils.store_data_saver import StoreDataSaver
from src.service.crawl.utils.crawling_manager import CrawlingManager


class NaverMapContentCrawler:
    """ë„¤ì´ë²„ ì§€ë„ ì½˜í…ì¸ (ë†€ê±°ë¦¬) ê²€ìƒ‰ í¬ë¡¤ë§ í´ë˜ìŠ¤ (ëª©ë¡ í´ë¦­ ë°©ì‹)"""
    
    CONTENT_KEYWORDS = [
        "ì„œìš¸ ë¯¸ìˆ ê´€",
        "ì„œìš¸ ë™ë¬¼ì¹´í˜",
        "ì„œìš¸ ê³µë°©",
        "ì„œìš¸ ì‚¬ê²©ì¥",
        "ì„œìš¸ ê·¼êµìœ ì ì§€",
        "ì„œìš¸ ë°•ë¬¼ê´€",
        "ì„œìš¸ í´ë¼ì´ë°",
    ]
    
    RESTART_INTERVAL = 30  # 30ê°œë§ˆë‹¤ ì»¨í…ìŠ¤íŠ¸ ì¬ì‹œì‘
    
    def __init__(self, headless: bool = False):
        self.logger = get_logger(__name__)
        self.headless = headless
        self.naver_map_url = "https://map.naver.com/v5/search"
        self.data_saver = StoreDataSaver()
        self.human_actions = HumanLikeActions()
        self.success_count = 0
        self.fail_count = 0
    
    async def crawl_by_keywords(self, keywords: list = None, delay: int = 20):
        """í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œ ë³‘ë ¬ í¬ë¡¤ë§ (ëª©ë¡ í´ë¦­ ë°©ì‹)"""
        keywords = keywords or self.CONTENT_KEYWORDS
        
        async with async_playwright() as p:
            browser = await OptimizedBrowserManager.create_optimized_browser(p, self.headless)
            
            try:
                self.logger.info(f"\n{'='*70}")
                self.logger.info(f"ğŸ“Š ì´ {len(keywords)}ê°œ í‚¤ì›Œë“œ í¬ë¡¤ë§ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬ + ëª©ë¡ í´ë¦­)")
                self.logger.info(f"{'='*70}\n")
                
                for keyword_idx, keyword in enumerate(keywords, 1):
                    self.logger.info(f"\n{'='*70}")
                    self.logger.info(f"[í‚¤ì›Œë“œ {keyword_idx}/{len(keywords)}] '{keyword}' í¬ë¡¤ë§ ì‹œì‘")
                    self.logger.info(f"{'='*70}\n")
                    
                    # í‚¤ì›Œë“œë³„ë¡œ í˜ì´ì§€ ë‹¨ìœ„ ì²˜ë¦¬
                    await self._crawl_keyword_by_pages(browser, keyword, delay)
                    
                    self.logger.info(f"âœ… [í‚¤ì›Œë“œ {keyword_idx}/{len(keywords)}] '{keyword}' ì™„ë£Œ\n")
                    
                    if keyword_idx < len(keywords):
                        import random
                        rest_time = random.uniform(40, 60)
                        self.logger.info(f"ğŸ›Œ í‚¤ì›Œë“œ ì™„ë£Œ, {rest_time:.0f}ì´ˆ íœ´ì‹...\n")
                        await asyncio.sleep(rest_time)
                
                self.logger.info(f"\n{'='*70}")
                self.logger.info(f"âœ… ëª¨ë“  í‚¤ì›Œë“œ í¬ë¡¤ë§ ì™„ë£Œ!")
                self.logger.info(f"   ì„±ê³µ: {self.success_count}ê°œ")
                self.logger.info(f"   ì‹¤íŒ¨: {self.fail_count}ê°œ")
                self.logger.info(f"{'='*70}\n")
                
            except Exception as e:
                self.logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
            finally:
                await browser.close()
    
    async def _crawl_keyword_by_pages(self, browser, keyword: str, delay: int):
        """
        í‚¤ì›Œë“œë³„ë¡œ í˜ì´ì§€ ë‹¨ìœ„ë¡œ í¬ë¡¤ë§
        ê° í˜ì´ì§€ë§ˆë‹¤ ë°°ì¹˜ ì²˜ë¦¬
        """
        # 1ë‹¨ê³„: ì „ì²´ í˜ì´ì§€ ìˆ˜ ë° ì•„ì´í…œ ê°œìˆ˜ íŒŒì•…
        total_items, total_pages = await self._get_total_items_count(browser, keyword)
        
        if total_items == 0:
            self.logger.warning(f"'{keyword}' ê²°ê³¼ ì—†ìŒ")
            return
        
        self.logger.info(f"âœ… '{keyword}' ì´ {total_items}ê°œ ({total_pages}í˜ì´ì§€)")
        
        # 2ë‹¨ê³„: ë°°ì¹˜ ë‹¨ìœ„ë¡œ í˜ì´ì§€ í¬ë¡¤ë§
        items_processed = 0
        
        for batch_start in range(0, total_items, self.RESTART_INTERVAL):
            batch_end = min(batch_start + self.RESTART_INTERVAL, total_items)
            
            batch_num = batch_start // self.RESTART_INTERVAL + 1
            total_batches = (total_items + self.RESTART_INTERVAL - 1) // self.RESTART_INTERVAL
            
            self.logger.info(f"\nğŸ”„ [{keyword}] ë°°ì¹˜ {batch_num}/{total_batches}: {batch_start+1}~{batch_end}/{total_items}")
            
            # ìƒˆ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = await OptimizedBrowserManager.create_stealth_context(browser)
            page = await context.new_page()
            
            try:
                # ë°°ì¹˜ ì²˜ë¦¬ (ëª©ë¡ í´ë¦­ ë°©ì‹)
                items_processed = await self._process_batch_with_click(
                    page, keyword, batch_start, batch_end, total_items, delay
                )
                
            except Exception as e:
                self.logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
            finally:
                await context.close()
                await asyncio.sleep(3)
                
                if batch_end < total_items:
                    import random
                    rest_time = random.uniform(20, 40)
                    self.logger.info(f"ğŸ›Œ ë°°ì¹˜ ì™„ë£Œ, {rest_time:.0f}ì´ˆ íœ´ì‹...\n")
                    await asyncio.sleep(rest_time)
    
    async def _get_total_items_count(self, browser, keyword: str) -> tuple:
        """
        ì „ì²´ ì•„ì´í…œ ê°œìˆ˜ ë° í˜ì´ì§€ ìˆ˜ íŒŒì•…
        
        Returns:
            Tuple[int, int]: (ì „ì²´ ì•„ì´í…œ ìˆ˜, ì „ì²´ í˜ì´ì§€ ìˆ˜)
        """
        context = await browser.new_context()
        page = await context.new_page()
        
        try:
            self.logger.info(f"  '{keyword}' ì „ì²´ ê°œìˆ˜ í™•ì¸ ì¤‘...")
            
            await page.goto(self.naver_map_url, wait_until='domcontentloaded')
            await asyncio.sleep(3)
            
            # ê²€ìƒ‰
            search_input_selector = '.input_search'
            await page.wait_for_selector(search_input_selector)
            await page.fill(search_input_selector, '')
            await asyncio.sleep(0.5)
            
            await page.fill(search_input_selector, keyword)
            await page.press(search_input_selector, 'Enter')
            await asyncio.sleep(5)
            
            # searchIframe ëŒ€ê¸°
            await page.wait_for_selector('iframe#searchIframe', timeout=10000)
            search_frame_locator = page.frame_locator('iframe#searchIframe')
            search_frame = page.frame('searchIframe')
            
            if not search_frame:
                return 0, 0
            
            await asyncio.sleep(3)
            
            # í˜ì´ì§€ë³„ë¡œ ìŠ¤í¬ë¡¤í•˜ì—¬ ì „ì²´ ê°œìˆ˜ íŒŒì•…
            total_items = 0
            page_num = 1
            
            while True:
                # í˜„ì¬ í˜ì´ì§€ ìŠ¤í¬ë¡¤
                item_count = await SearchResultScroller.scroll_current_page(
                    search_frame_locator=search_frame_locator,
                    search_frame=search_frame
                )
                
                if item_count == 0:
                    break
                
                total_items += item_count
                
                # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸
                has_next = await PageNavigator.go_to_next_page_naver(
                    search_frame_locator=search_frame_locator,
                    search_frame=search_frame
                )
                
                if not has_next:
                    break
                
                page_num += 1
                await asyncio.sleep(2)
            
            return total_items, page_num
            
        except Exception as e:
            self.logger.error(f"'{keyword}' ì „ì²´ ê°œìˆ˜ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return 0, 0
        finally:
            await context.close()
    
    async def _process_batch_with_click(
        self,
        page: Page,
        keyword: str,
        batch_start: int,
        batch_end: int,
        total: int,
        delay: int
    ):
        """
        ë°°ì¹˜ ë‹¨ìœ„ ë³‘ë ¬ í¬ë¡¤ë§ (ëª©ë¡ í´ë¦­ ë°©ì‹)
        """
        try:
            # ë„¤ì´ë²„ ì§€ë„ ê²€ìƒ‰
            await page.goto(self.naver_map_url, wait_until='domcontentloaded')
            await asyncio.sleep(3)
            
            search_input_selector = '.input_search'
            await page.wait_for_selector(search_input_selector)
            await page.fill(search_input_selector, '')
            await asyncio.sleep(0.5)
            
            await page.fill(search_input_selector, keyword)
            await page.press(search_input_selector, 'Enter')
            await asyncio.sleep(5)
            
            # searchIframe ëŒ€ê¸°
            await page.wait_for_selector('iframe#searchIframe', timeout=10000)
            search_frame_locator = page.frame_locator('iframe#searchIframe')
            search_frame = page.frame('searchIframe')
            
            if not search_frame:
                self.logger.error("searchIframeì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return 0
            
            await asyncio.sleep(3)
            
            # batch_start ìœ„ì¹˜ê¹Œì§€ í˜ì´ì§€ ì´ë™ ë° ìŠ¤í¬ë¡¤
            current_idx = 0
            current_page = 1
            
            while current_idx < batch_end:
                # í˜„ì¬ í˜ì´ì§€ ìŠ¤í¬ë¡¤
                await SearchResultScroller.scroll_current_page(
                    search_frame_locator=search_frame_locator,
                    search_frame=search_frame
                )
                
                # í˜„ì¬ í˜ì´ì§€ì˜ ì•„ì´í…œ ê°œìˆ˜
                item_selector = '#_pcmap_list_scroll_container > ul > li'
                items = await search_frame_locator.locator(item_selector).all()
                items_in_page = len(items)
                
                # ì´ í˜ì´ì§€ì—ì„œ í¬ë¡¤ë§í•  ì•„ì´í…œ ë²”ìœ„ ê³„ì‚°
                page_start = max(0, batch_start - current_idx)
                page_end = min(items_in_page, batch_end - current_idx)
                
                # í¬ë¡¤ë§í•  ì•„ì´í…œì´ ì´ í˜ì´ì§€ì— ìˆìœ¼ë©´
                if page_start < items_in_page and current_idx < batch_end:
                    # ì´ í˜ì´ì§€ì˜ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
                    batch_items = []
                    
                    # ========================================
                    # ğŸ”¥ ìˆ˜ì •: ì•„ì´í…œëª… ë¯¸ë¦¬ ì¶”ì¶œ
                    # ========================================
                    for idx in range(page_start, page_end):
                        if current_idx + idx >= batch_start and current_idx + idx < batch_end:
                            try:
                                # ì•„ì´í…œ ìš”ì†Œ ê°€ì ¸ì˜¤ê¸°
                                item = items[idx]
                                
                                # ì•„ì´í…œ ì´ë¦„ ì¶”ì¶œ
                                item_name = await self._extract_item_name(item, idx, items_in_page)
                                
                                batch_items.append({
                                    'page_idx': idx,
                                    'global_idx': current_idx + idx,
                                    'page_num': current_page,
                                    'name': item_name  # âœ… ì´ë¦„ ì¶”ê°€!
                                })
                            except Exception as e:
                                self.logger.warning(f"ì•„ì´í…œ {idx} ì´ë¦„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                                # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (ì´ë¦„ ì—†ì´)
                                batch_items.append({
                                    'page_idx': idx,
                                    'global_idx': current_idx + idx,
                                    'page_num': current_page,
                                    'name': f"ì•„ì´í…œ {current_idx + idx + 1}"
                                })
                    
                    if batch_items:
                        # ========================================
                        # ğŸ”¥ ë³‘ë ¬ ì²˜ë¦¬: CrawlingManager ì‚¬ìš©
                        # ========================================
                        crawling_manager = CrawlingManager("ì½˜í…ì¸ ")
                        
                        await crawling_manager.execute_crawling_with_save(
                            stores=batch_items,
                            crawl_func=lambda item, i, t: self._crawl_single_item_from_list(
                                page, search_frame_locator, item_selector, item, total
                            ),
                            save_func=self._save_wrapper_with_total(total),
                            delay=delay
                        )
                        
                        # ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
                        self.success_count += crawling_manager.success_count
                        self.fail_count += crawling_manager.fail_count
                
                # í˜„ì¬ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
                current_idx += items_in_page
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ (í•„ìš”í•œ ê²½ìš°)
                if current_idx < batch_end:
                    has_next = await PageNavigator.go_to_next_page_naver(
                        search_frame_locator=search_frame_locator,
                        search_frame=search_frame
                    )
                    
                    if not has_next:
                        self.logger.warning(f"ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ (í˜„ì¬: {current_idx}/{batch_end})")
                        break
                    
                    current_page += 1
                    await asyncio.sleep(3)
                else:
                    break
            
            return current_idx
            
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return 0
    
    async def _crawl_single_item_from_list(
        self,
        page: Page,
        search_frame_locator,
        item_selector: str,
        item_info: dict,
        total: int
    ):
        """
        ëª©ë¡ì—ì„œ ë‹¨ì¼ ì•„ì´í…œ í´ë¦­í•˜ì—¬ í¬ë¡¤ë§ (ë³‘ë ¬ìš©)
        
        Args:
            page: ë©”ì¸ í˜ì´ì§€
            search_frame_locator: ê²€ìƒ‰ iframe locator
            item_selector: ì•„ì´í…œ ì„ íƒì
            item_info: ì•„ì´í…œ ì •ë³´ (page_idx, global_idx í¬í•¨)
            total: ì „ì²´ ê°œìˆ˜
            
        Returns:
            Tuple: (store_data, name) ë˜ëŠ” None
        """
        page_idx = item_info['page_idx']
        global_idx = item_info['global_idx']
        
        try:
            # ë§¤ë²ˆ ëª©ë¡ ìƒˆë¡œ ê°€ì ¸ì˜¤ê¸° (DOM ë³€ê²½ ëŒ€ì‘)
            items = await search_frame_locator.locator(item_selector).all()
            
            if page_idx >= len(items):
                self.logger.error(f"[{global_idx+1}/{total}] ì¸ë±ìŠ¤ ë²”ìœ„ ì´ˆê³¼: {page_idx}/{len(items)}")
                return None
            
            item = items[page_idx]
            
            # ì•„ì´í…œ ì´ë¦„ ì¶”ì¶œ
            name = await self._extract_item_name(item, page_idx, len(items))
            
            # í´ë¦­ ìš”ì†Œ ì°¾ê¸°
            click_element = await self._find_click_element(item, page_idx)
            
            if not click_element:
                self.logger.error(f"[{global_idx+1}/{total}] '{name}' í´ë¦­ ìš”ì†Œ ì—†ìŒ")
                return None
            
            # ì‚¬ëŒì²˜ëŸ¼ í´ë¦­
            await self.human_actions.human_like_click(click_element)
            await asyncio.sleep(3)
            
            # entryIframe ëŒ€ê¸°
            try:
                await page.wait_for_selector('iframe#entryIframe', timeout=10000)
                entry_frame = page.frame_locator('iframe#entryIframe')
                await asyncio.sleep(3)
                
                # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                extractor = StoreDetailExtractor(entry_frame, page)
                store_data = await extractor.extract_all_details()
                
                if store_data:
                    actual_name = store_data[0]
                    
                    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
                    await OptimizedBrowserManager.clear_page_resources(page)
                    
                    return (store_data, actual_name)
                else:
                    self.logger.error(f"[{global_idx+1}/{total}] '{name}' ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
                    return None
                    
            except TimeoutError:
                self.logger.error(f"[{global_idx+1}/{total}] '{name}' entryIframe íƒ€ì„ì•„ì›ƒ")
                return None
                
        except Exception as e:
            self.logger.error(f"[{global_idx+1}/{total}] í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _save_wrapper_with_total(self, total: int):
        """ì €ì¥ ë˜í¼ íŒ©í† ë¦¬"""
        async def wrapper(idx: int, _, store_data_tuple, store_name: str):
            if store_data_tuple is None:
                return (False, "í¬ë¡¤ë§ ì‹¤íŒ¨")
            
            store_data, actual_name = store_data_tuple
            
            return await self.data_saver.save_store_data(
                idx=idx,
                total=total,
                store_data=store_data,
                store_name=actual_name,
                log_prefix="ì½˜í…ì¸ "
            )
        
        return wrapper
    
    async def _extract_item_name(self, item, idx: int, item_count: int) -> str:
        """ì•„ì´í…œ ì´ë¦„ ì¶”ì¶œ (4ê°€ì§€ ì„ íƒì ì‹œë„)"""
        name_selectors = [
            'div.Dr2xO > div.pIwpC > a > span.CMy2_',
            'div.qbGlu > div.ouxiq > div.ApCpt > a > span.YwYLL',
            'div.Np1CD > div:nth-child(2) > div.SbNoJ > a > span.t3s7S',
            'div.Np1CD > div > div.SbNoJ > a > span.t3s7S',
        ]
        
        for selector in name_selectors:
            try:
                name_element = item.locator(selector).first
                if await name_element.count() > 0:
                    name = await name_element.inner_text(timeout=2000)
                    if name and name.strip():
                        return name.strip()
            except:
                continue
        
        return f"ì•„ì´í…œ {idx+1}"
    
    async def _find_click_element(self, item, idx: int):
        """í´ë¦­ ìš”ì†Œ ì°¾ê¸° (4ê°€ì§€ ì„ íƒì ì‹œë„)"""
        link_selectors = [
            'div.Dr2xO > div.pIwpC > a',
            'div.qbGlu > div.ouxiq > div.ApCpt > a',
            'div.Np1CD > div:nth-child(2) > div.SbNoJ > a',
            'div.Np1CD > div > div.SbNoJ > a',
        ]
        
        for selector in link_selectors:
            try:
                element = item.locator(selector).first
                if await element.count() > 0:
                    return element
            except:
                continue
        
        # ëª¨ë‘ ì‹¤íŒ¨í•˜ë©´ ì•„ì´í…œ ì „ì²´ ë°˜í™˜
        return item


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger = get_logger(__name__)
    
    logger.info("="*70)
    logger.info("ğŸš€ ë„¤ì´ë²„ ì§€ë„ ì½˜í…ì¸  í¬ë¡¤ëŸ¬ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬ + ëª©ë¡ í´ë¦­)")
    logger.info("="*70)
    
    try:
        crawler = NaverMapContentCrawler(headless=False)
        
        await crawler.crawl_by_keywords(
            keywords=None,
            delay=15
        )
        
        logger.info("="*70)
        logger.info("ğŸ í¬ë¡¤ëŸ¬ ì¢…ë£Œ")
        logger.info("="*70)
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())