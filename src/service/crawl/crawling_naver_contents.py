"""
ë„¤ì´ë²„ ì§€ë„ ì½˜í…ì¸ (ë†€ê±°ë¦¬) ê²€ìƒ‰ í¬ë¡¤ë§ ëª¨ë“ˆ (ë©”ëª¨ë¦¬ ìµœì í™” + ë´‡ ìš°íšŒ + ë³‘ë ¬ ì²˜ë¦¬)
"""
import asyncio
from playwright.async_api import async_playwright, TimeoutError, Page
import sys, os
from dotenv import load_dotenv

load_dotenv(dotenv_path="src/.env")

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from src.logger.custom_logger import get_logger

# ê³µí†µ ëª¨ë“ˆ import
from src.service.crawl.utils.optimized_browser_manager import OptimizedBrowserManager
from src.service.crawl.utils.human_like_actions import HumanLikeActions
from src.service.crawl.utils.scroll_helper import SearchResultScroller, PageNavigator
from src.service.crawl.utils.store_detail_extractor import StoreDetailExtractor
from src.service.crawl.utils.store_data_saver import StoreDataSaver
from src.service.crawl.utils.crawling_manager import CrawlingManager


class NaverMapContentCrawler:
    """ë„¤ì´ë²„ ì§€ë„ ì½˜í…ì¸ (ë†€ê±°ë¦¬) ê²€ìƒ‰ í¬ë¡¤ë§ í´ë˜ìŠ¤"""
    
    CONTENT_KEYWORDS = [
        "ì„œìš¸ ë™ë¬¼ì¹´í˜",
        "ì„œìš¸ ê³µë°©",
        "ì„œìš¸ ì‚¬ê²©ì¥",
        "ì„œìš¸ ë¯¸ìˆ ê´€",
        "ì„œìš¸ ê·¼êµìœ ì ì§€",
        "ì„œìš¸ ë°•ë¬¼ê´€",
        "ì„œìš¸ í´ë¼ì´ë°",
    ]
    
    RESTART_INTERVAL = 30  # 30ê°œë§ˆë‹¤ ì»¨í…ìŠ¤íŠ¸ ì¬ì‹œì‘
    
    def __init__(self, headless: bool = False):
        self.logger = get_logger(__name__)
        self.headless = headless
        self.naver_map_url = "https://map.naver.com/v5/search"
        self.data_saver = StoreDataSaver()
        self.human_actions = HumanLikeActions()
        self.success_count = 0
        self.fail_count = 0
    
    async def crawl_by_keywords(self, keywords: list = None, delay: int = 20):
        """í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œ ë³‘ë ¬ í¬ë¡¤ë§"""
        keywords = keywords or self.CONTENT_KEYWORDS
        
        async with async_playwright() as p:
            browser = await OptimizedBrowserManager.create_optimized_browser(p, self.headless)
            
            try:
                self.logger.info(f"\n{'='*70}")
                self.logger.info(f"ğŸ“Š ì´ {len(keywords)}ê°œ í‚¤ì›Œë“œ í¬ë¡¤ë§ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)")
                self.logger.info(f"{'='*70}\n")
                
                for keyword_idx, keyword in enumerate(keywords, 1):
                    self.logger.info(f"\n{'='*70}")
                    self.logger.info(f"[í‚¤ì›Œë“œ {keyword_idx}/{len(keywords)}] '{keyword}' í¬ë¡¤ë§ ì‹œì‘")
                    self.logger.info(f"{'='*70}\n")
                    
                    # ì „ì²´ ì•„ì´í…œ ëª©ë¡ ìˆ˜ì§‘
                    all_items = await self._collect_items_by_keyword(browser, keyword)
                    
                    if not all_items:
                        self.logger.warning(f"'{keyword}' ê²°ê³¼ ì—†ìŒ")
                        continue
                    
                    total = len(all_items)
                    self.logger.info(f"âœ… '{keyword}' ì´ {total}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                    
                    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë³‘ë ¬ í¬ë¡¤ë§
                    for batch_start in range(0, total, self.RESTART_INTERVAL):
                        batch_end = min(batch_start + self.RESTART_INTERVAL, total)
                        batch = all_items[batch_start:batch_end]
                        
                        batch_num = batch_start // self.RESTART_INTERVAL + 1
                        total_batches = (total + self.RESTART_INTERVAL - 1) // self.RESTART_INTERVAL
                        
                        self.logger.info(f"\nğŸ”„ [{keyword}] ë°°ì¹˜ {batch_num}/{total_batches}: {batch_start+1}~{batch_end}/{total}")
                        
                        context = await OptimizedBrowserManager.create_stealth_context(browser)
                        page = await context.new_page()
                        
                        try:
                            await self._process_batch_parallel(
                                page, keyword, batch, batch_start, total, delay
                            )
                        except Exception as e:
                            self.logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            import traceback
                            self.logger.error(traceback.format_exc())
                        finally:
                            await context.close()
                            await asyncio.sleep(3)
                            
                            if batch_end < total:
                                import random
                                rest_time = random.uniform(20, 40)
                                self.logger.info(f"ğŸ›Œ ë°°ì¹˜ ì™„ë£Œ, {rest_time:.0f}ì´ˆ íœ´ì‹...\n")
                                await asyncio.sleep(rest_time)
                    
                    self.logger.info(f"âœ… [í‚¤ì›Œë“œ {keyword_idx}/{len(keywords)}] '{keyword}' ì™„ë£Œ\n")
                    
                    if keyword_idx < len(keywords):
                        import random
                        rest_time = random.uniform(40, 60)
                        self.logger.info(f"ğŸ›Œ í‚¤ì›Œë“œ ì™„ë£Œ, {rest_time:.0f}ì´ˆ íœ´ì‹...\n")
                        await asyncio.sleep(rest_time)
                
                self.logger.info(f"\n{'='*70}")
                self.logger.info(f"âœ… ëª¨ë“  í‚¤ì›Œë“œ í¬ë¡¤ë§ ì™„ë£Œ!")
                self.logger.info(f"   ì„±ê³µ: {self.success_count}ê°œ")
                self.logger.info(f"   ì‹¤íŒ¨: {self.fail_count}ê°œ")
                self.logger.info(f"{'='*70}\n")
                
            except Exception as e:
                self.logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
            finally:
                await browser.close()
    
    async def _collect_items_by_keyword(self, browser, keyword: str) -> list:
        """í‚¤ì›Œë“œë¡œ ì „ì²´ ì•„ì´í…œ ëª©ë¡ ìˆ˜ì§‘"""
        context = await browser.new_context()
        page = await context.new_page()
        
        all_items = []
        
        try:
            await page.goto(self.naver_map_url, wait_until='domcontentloaded')
            await asyncio.sleep(3)
            
            search_input_selector = '.input_search'
            await page.wait_for_selector(search_input_selector)
            await page.fill(search_input_selector, '')
            await asyncio.sleep(0.5)
            
            await page.fill(search_input_selector, keyword)
            await page.press(search_input_selector, 'Enter')
            await asyncio.sleep(5)
            
            await page.wait_for_selector('iframe#searchIframe', timeout=10000)
            search_frame_locator = page.frame_locator('iframe#searchIframe')
            search_frame = page.frame('searchIframe')
            
            if not search_frame:
                return []
            
            await asyncio.sleep(3)
            
            page_num = 1
            
            while True:
                self.logger.info(f"  '{keyword}' {page_num}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
                
                item_count = await SearchResultScroller.scroll_current_page(
                    search_frame_locator=search_frame_locator,
                    search_frame=search_frame
                )
                
                if item_count == 0:
                    break
                
                item_selector = '#_pcmap_list_scroll_container > ul > li'
                items = await search_frame_locator.locator(item_selector).all()
                
                for idx, item in enumerate(items):
                    try:
                        name = await self._extract_item_name(item, idx, len(items))
                        
                        all_items.append({
                            'name': name,
                            'keyword': keyword,
                            'page': page_num,
                            'idx': idx
                        })
                        
                    except Exception as e:
                        self.logger.warning(f"  ì•„ì´í…œ {idx+1} ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        continue
                
                self.logger.info(f"  '{keyword}' {page_num}í˜ì´ì§€: {len(items)}ê°œ ìˆ˜ì§‘ (ëˆ„ì  {len(all_items)}ê°œ)")
                
                has_next = await PageNavigator.go_to_next_page_naver(
                    search_frame_locator=search_frame_locator,
                    search_frame=search_frame
                )
                
                if not has_next:
                    self.logger.info(f"  '{keyword}' ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬")
                    break
                
                page_num += 1
                await asyncio.sleep(3)
                
        except Exception as e:
            self.logger.error(f"'{keyword}' ëª©ë¡ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
        finally:
            await context.close()
        
        return all_items
    
    async def _process_batch_parallel(
        self, 
        page: Page, 
        keyword: str,
        batch: list, 
        batch_start: int, 
        total: int, 
        delay: int
    ):
        """ë°°ì¹˜ ë³‘ë ¬ í¬ë¡¤ë§"""
        try:
            # ========================================
            # ğŸ”¥ ë³‘ë ¬ ì²˜ë¦¬: CrawlingManager ì‚¬ìš©
            # ========================================
            crawling_manager = CrawlingManager("ì½˜í…ì¸ ")
            
            await crawling_manager.execute_crawling_with_save(
                stores=batch,
                crawl_func=lambda item, i, t: self._search_and_crawl_item(
                    page, item['name'], keyword
                ),
                save_func=self._save_wrapper_with_total(batch_start, total),
                delay=delay
            )
            
            # ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
            self.success_count += crawling_manager.success_count
            self.fail_count += crawling_manager.fail_count
            
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
    
    async def _search_and_crawl_item(self, page: Page, name: str, keyword: str):
        """ë„¤ì´ë²„ ì§€ë„ì—ì„œ ê²€ìƒ‰í•˜ì—¬ í¬ë¡¤ë§"""
        try:
            await page.goto(self.naver_map_url, wait_until='domcontentloaded')
            await asyncio.sleep(2)
            
            search_query = f"{keyword} {name}"
            search_input_selector = '.input_search'
            await page.wait_for_selector(search_input_selector)
            await page.fill(search_input_selector, '')
            await asyncio.sleep(0.5)
            
            await page.fill(search_input_selector, search_query)
            await page.press(search_input_selector, 'Enter')
            await asyncio.sleep(3)
            
            await page.wait_for_selector('iframe#entryIframe', timeout=10000)
            entry_frame = page.frame_locator('iframe#entryIframe')
            await asyncio.sleep(3)
            
            extractor = StoreDetailExtractor(entry_frame, page)
            store_data = await extractor.extract_all_details()
            
            if store_data:
                actual_name = store_data[0]
                
                # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
                await OptimizedBrowserManager.clear_page_resources(page)
                
                return (store_data, actual_name)
            
            return None
            
        except TimeoutError:
            return None
        except Exception as e:
            self.logger.error(f"'{name}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _save_wrapper_with_total(self, batch_start: int, total: int):
        """ì €ì¥ ë˜í¼ íŒ©í† ë¦¬ (total í¬í•¨)"""
        async def wrapper(idx: int, _, store_data_tuple, store_name: str):
            if store_data_tuple is None:
                return (False, "í¬ë¡¤ë§ ì‹¤íŒ¨")
            
            store_data, actual_name = store_data_tuple
            global_idx = batch_start + idx
            
            return await self.data_saver.save_store_data(
                idx=global_idx,
                total=total,
                store_data=store_data,
                store_name=actual_name,
                log_prefix="ì½˜í…ì¸ "
            )
        
        return wrapper
    
    async def _extract_item_name(self, item, idx: int, item_count: int) -> str:
        """ì•„ì´í…œ ì´ë¦„ ì¶”ì¶œ"""
        name_selectors = [
            'div.Dr2xO > div.pIwpC > a > span.CMy2_',
            'div.qbGlu > div.ouxiq > div.ApCpt > a > span.YwYLL',
            'div.Np1CD > div:nth-child(2) > div.SbNoJ > a > span.t3s7S',
            'div.Np1CD > div > div.SbNoJ > a > span.t3s7S',
        ]
        
        for selector in name_selectors:
            try:
                name_element = item.locator(selector).first
                if await name_element.count() > 0:
                    name = await name_element.inner_text(timeout=2000)
                    if name and name.strip():
                        return name.strip()
            except:
                continue
        
        return f"ì•„ì´í…œ {idx+1}"


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger = get_logger(__name__)
    
    logger.info("="*70)
    logger.info("ğŸš€ ë„¤ì´ë²„ ì§€ë„ ì½˜í…ì¸  í¬ë¡¤ëŸ¬ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)")
    logger.info("="*70)
    
    try:
        crawler = NaverMapContentCrawler(headless=False)
        
        await crawler.crawl_by_keywords(
            keywords=None,
            delay=15
        )
        
        logger.info("="*70)
        logger.info("ğŸ í¬ë¡¤ëŸ¬ ì¢…ë£Œ")
        logger.info("="*70)
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())