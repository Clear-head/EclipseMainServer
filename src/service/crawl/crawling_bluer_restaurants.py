import json
import asyncio
from playwright.async_api import async_playwright, TimeoutError, Page
import re
from typing import Optional, List, Tuple
import sys, os
import datetime
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path="src/.env")

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from src.logger.logger_handler import get_logger
from src.service.crawl.utils.store_detail_extractor import StoreDetailExtractor
from src.service.crawl.utils.store_data_saver import StoreDataSaver

# ë¡œê±° ì´ˆê¸°í™”
logger = get_logger('crawling_bluer')

class BluerRestaurantCrawler:
    """Bluer ì›¹ì‚¬ì´íŠ¸ ìŒì‹ì  í¬ë¡¤ë§ í´ë˜ìŠ¤"""
    
    def __init__(self, headless: bool = False):
        """
        Args:
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
        """
        self.headless = headless
        self.bluer_url = "https://www.bluer.co.kr/search?query=&foodType=&foodTypeDetail=&feature=112&location=&locationDetail=&area=&areaDetail=&ribbonType=&priceRangeMin=0&priceRangeMax=1000&week=&hourMin=0&hourMax=48&year=&evaluate=&sort=&listType=card&isSearchName=false&isBrand=false&isAround=false&isMap=false&zone1=&zone2=&food1=&food2=&zone2Lat=&zone2Lng=&distance=1000&isMapList=false#restaurant-filter-bottom"
        self.naver_map_url = "https://map.naver.com/v5/search"
        self.data_saver = StoreDataSaver()
        
        logger.info(f"âœ“ Bluer í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def crawl_all_pages(self, delay: int = 5, naver_delay: int = 20):
        """
        Bluer ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§
        
        Args:
            delay: Bluer í˜ì´ì§€ ê°„ ë”œë ˆì´ (ì´ˆ)
            naver_delay: ë„¤ì´ë²„ í¬ë¡¤ë§ ê°„ ë”œë ˆì´ (ì´ˆ)
        """
        async with async_playwright() as p:
            # Bluer í¬ë¡¤ë§ìš© ë¸Œë¼ìš°ì €
            bluer_browser = await p.chromium.launch(headless=self.headless)
            bluer_page = await bluer_browser.new_page()
            
            # ë„¤ì´ë²„ í¬ë¡¤ë§ìš© ë¸Œë¼ìš°ì €
            naver_browser = await p.chromium.launch(
                headless=self.headless,
                args=['--enable-features=ClipboardAPI']
            )
            naver_context = await naver_browser.new_context(
                permissions=['clipboard-read', 'clipboard-write']
            )
            naver_page = await naver_context.new_page()
            
            try:
                # Bluer í˜ì´ì§€ë¡œ ì´ë™
                logger.info(f"Bluer í˜ì´ì§€ ì ‘ì† ì¤‘...")
                await bluer_page.goto(self.bluer_url, wait_until='networkidle')
                await asyncio.sleep(3)
                
                # ì „ì²´ ìŒì‹ì  ëª©ë¡ ì €ì¥
                all_restaurants = []
                current_page = 1
                
                while True:
                    logger.info(f"=" * 60)
                    logger.info(f"ğŸ“„ í˜ì´ì§€ {current_page} í¬ë¡¤ë§ ì‹œì‘")
                    logger.info(f"=" * 60)
                    
                    # í˜„ì¬ í˜ì´ì§€ì˜ ìŒì‹ì  ëª©ë¡ ì¶”ì¶œ
                    restaurants = await self._extract_restaurants_from_page(bluer_page)
                    
                    if restaurants:
                        logger.info(f"í˜ì´ì§€ {current_page}ì—ì„œ {len(restaurants)}ê°œ ìŒì‹ì  ë°œê²¬")
                        all_restaurants.extend(restaurants)
                    else:
                        logger.warning(f"í˜ì´ì§€ {current_page}ì—ì„œ ìŒì‹ì ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    
                    # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ í™•ì¸ ë° í´ë¦­
                    has_next = await self._click_next_page(bluer_page)
                    
                    if not has_next:
                        logger.info("=" * 60)
                        logger.info(f"âœ“ ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬! ì´ {len(all_restaurants)}ê°œ ìŒì‹ì  ìˆ˜ì§‘ ì™„ë£Œ")
                        logger.info("=" * 60)
                        break
                    
                    current_page += 1
                    await asyncio.sleep(delay)
                
                # ë„¤ì´ë²„ ì§€ë„ì—ì„œ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ë° ì €ì¥
                await self._crawl_naver_details(naver_page, all_restaurants, naver_delay)
                
            except Exception as e:
                logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                logger.error(traceback.format_exc())
            finally:
                await bluer_page.close()
                await bluer_browser.close()
                await naver_context.close()
                await naver_browser.close()
    
    async def _extract_restaurants_from_page(self, page: Page) -> List[Tuple[str, str]]:
        """
        í˜„ì¬ í˜ì´ì§€ì—ì„œ ìŒì‹ì  ì´ë¦„ê³¼ ì£¼ì†Œ ì¶”ì¶œ
        
        Returns:
            List[Tuple[str, str]]: [(ìŒì‹ì ëª…, ì£¼ì†Œ), ...]
        """
        restaurants = []
        
        try:
            # ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            await page.wait_for_selector('#list-restaurant', timeout=10000)
            await asyncio.sleep(2)
            
            # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ê°œìˆ˜ í™•ì¸
            list_items = await page.locator('#list-restaurant > li').all()
            logger.info(f"í˜„ì¬ í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ìˆ˜: {len(list_items)}")
            
            for idx, item in enumerate(list_items, 1):
                try:
                    # ìŒì‹ì ëª… ì¶”ì¶œ
                    name_selector = 'div > header > div.header-title > div:nth-child(2) > h3'
                    name_element = item.locator(name_selector)
                    
                    if await name_element.count() > 0:
                        name = await name_element.inner_text(timeout=3000)
                        name = name.strip()
                    else:
                        logger.warning(f"ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ {idx}: ìŒì‹ì ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue
                    
                    # ì£¼ì†Œ ì¶”ì¶œ
                    address_selector = 'div > div > div.info > div:nth-child(1) > div'
                    address_element = item.locator(address_selector)
                    
                    if await address_element.count() > 0:
                        address = await address_element.inner_text(timeout=3000)
                        address = address.strip()
                    else:
                        logger.warning(f"ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ {idx}: ì£¼ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        address = ""
                    
                    if name:
                        restaurants.append((name, address))
                        logger.info(f"  [{idx}] {name} - {address}")
                    
                except Exception as item_error:
                    logger.error(f"ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ {idx} ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {item_error}")
                    continue
            
        except TimeoutError:
            logger.error("ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Timeout)")
        except Exception as e:
            logger.error(f"ìŒì‹ì  ëª©ë¡ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return restaurants
    
    async def _click_next_page(self, page: Page) -> bool:
        """
        ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ í´ë¦­
        
        Returns:
            bool: ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆìœ¼ë©´ True, ì—†ìœ¼ë©´ False
        """
        try:
            # í˜ì´ì§€ ì„ íƒ ì˜ì—­ ëŒ€ê¸°
            await page.wait_for_selector('#page-selection > ul', timeout=5000)
            await asyncio.sleep(1)
            
            # active í´ë˜ìŠ¤ë¥¼ ê°€ì§„ í˜„ì¬ í˜ì´ì§€ ì°¾ê¸°
            page_items = await page.locator('#page-selection > ul > li').all()
            
            active_index = -1
            for idx, item in enumerate(page_items):
                class_attr = await item.get_attribute('class')
                if class_attr and 'active' in class_attr:
                    active_index = idx
                    break
            
            if active_index == -1:
                logger.warning("active í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ë‹¤ìŒ ë²„íŠ¼ì´ ìˆëŠ”ì§€ í™•ì¸
            next_index = active_index + 1
            if next_index >= len(page_items):
                logger.info("ë‹¤ìŒ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. (ë§ˆì§€ë§‰ í˜ì´ì§€)")
                return False
            
            # ë‹¤ìŒ ë²„íŠ¼ í´ë¦­
            next_button = page_items[next_index]
            await next_button.scroll_into_view_if_needed()
            await asyncio.sleep(0.5)
            
            # ë²„íŠ¼ ì•ˆì˜ a íƒœê·¸ ë˜ëŠ” ë²„íŠ¼ ìì²´ í´ë¦­
            clickable = next_button.locator('a, button').first
            if await clickable.count() > 0:
                await clickable.click()
            else:
                await next_button.click()
            
            logger.info(f"âœ“ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
            await asyncio.sleep(2)
            
            return True
            
        except TimeoutError:
            logger.error("í˜ì´ì§€ ì„ íƒ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            logger.error(f"ë‹¤ìŒ í˜ì´ì§€ í´ë¦­ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    async def _crawl_naver_details(self, naver_page: Page, restaurants: List[Tuple[str, str]], delay: int):
        """
        ë„¤ì´ë²„ ì§€ë„ì—ì„œ ìŒì‹ì  ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ë° ì €ì¥
        
        Args:
            naver_page: ë„¤ì´ë²„ í¬ë¡¤ë§ìš© í˜ì´ì§€
            restaurants: [(ìŒì‹ì ëª…, ì£¼ì†Œ), ...]
            delay: í¬ë¡¤ë§ ê°„ ë”œë ˆì´ (ì´ˆ)
        """
        total = len(restaurants)
        success_count = 0
        fail_count = 0
        
        logger.info("")
        logger.info("=" * 60)
        logger.info(f"ğŸ—ºï¸  ë„¤ì´ë²„ ì§€ë„ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘ (ì´ {total}ê°œ)")
        logger.info("=" * 60)
        
        save_tasks = []
        
        for idx, (name, address) in enumerate(restaurants, 1):
            logger.info(f"[ë„¤ì´ë²„ í¬ë¡¤ë§ {idx}/{total}] '{name}' í¬ë¡¤ë§ ì§„í–‰ ì¤‘...")
            logger.info(f"  - ì£¼ì†Œ: {address}")
            
            # ë„¤ì´ë²„ ì§€ë„ì—ì„œ ê²€ìƒ‰
            store_data = await self._search_naver_map(naver_page, name, address)
            
            if store_data:
                logger.info(f"[ë„¤ì´ë²„ í¬ë¡¤ë§ {idx}/{total}] '{name}' í¬ë¡¤ë§ ì™„ë£Œ")
                
                # ì €ì¥ íƒœìŠ¤í¬ ìƒì„± (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)
                save_task = asyncio.create_task(
                    self.data_saver.save_store_data(
                        idx=idx,
                        total=total,
                        store_data=store_data,
                        store_name=name,
                        log_prefix="Bluer"
                    )
                )
                save_tasks.append(save_task)
                
                # ë§ˆì§€ë§‰ ìƒì ì´ ì•„ë‹ˆë©´ ë”œë ˆì´
                if idx < total:
                    await asyncio.sleep(delay)
            else:
                fail_count += 1
                logger.error(f"[ë„¤ì´ë²„ í¬ë¡¤ë§ {idx}/{total}] '{name}' í¬ë¡¤ë§ ì‹¤íŒ¨")
                
                # ì‹¤íŒ¨í•´ë„ ë”œë ˆì´
                if idx < total:
                    await asyncio.sleep(delay)
        
        # ëª¨ë“  í¬ë¡¤ë§ì´ ëë‚œ í›„ ì €ì¥ íƒœìŠ¤í¬ë“¤ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        logger.info("=" * 60)
        logger.info(f"ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! ì €ì¥ ì‘ì—… ì™„ë£Œ ëŒ€ê¸° ì¤‘... ({len(save_tasks)}ê°œ)")
        logger.info("=" * 60)
        
        if save_tasks:
            save_results = await asyncio.gather(*save_tasks, return_exceptions=True)
            
            # ì €ì¥ ê²°ê³¼ ì§‘ê³„
            for result in save_results:
                if isinstance(result, Exception):
                    fail_count += 1
                elif isinstance(result, tuple):
                    success, msg = result
                    if success:
                        success_count += 1
                    else:
                        fail_count += 1
        
        logger.info("=" * 60)
        logger.info(f"ì „ì²´ ì‘ì—… ì™„ë£Œ: ì„±ê³µ {success_count}/{total}, ì‹¤íŒ¨ {fail_count}/{total}")
        logger.info("=" * 60)
    
    def _extract_road_name(self, address: str) -> str:
        """
        ì£¼ì†Œì—ì„œ ë„ë¡œëª…(~ë¡œ, ~ê¸¸)ê¹Œì§€ë§Œ ì¶”ì¶œ
        
        Args:
            address: ì „ì²´ ì£¼ì†Œ
            
        Returns:
            str: ~ë¡œ ë˜ëŠ” ~ê¸¸ê¹Œì§€ì˜ ì£¼ì†Œ
        """
        if not address:
            return ""
        
        address_parts = address.split()
        result_parts = []
        
        for part in address_parts:
            result_parts.append(part)
            
            # ~ë¡œ, ~ê¸¸ì´ ë‚˜ì˜¤ë©´ ë°”ë¡œ ì¢…ë£Œ
            if part.endswith('ë¡œ') or part.endswith('ê¸¸'):
                break
            
            # ì•ˆì „ì¥ì¹˜: ìµœëŒ€ 5ê°œ ìš”ì†Œê¹Œì§€
            if len(result_parts) >= 5:
                break
        
        return " ".join(result_parts)
    
    async def _search_naver_map(self, page: Page, store_name: str, store_address: str):
        """
        ë„¤ì´ë²„ ì§€ë„ì—ì„œ ê²€ìƒ‰ ë° ì •ë³´ ì¶”ì¶œ
        
        Args:
            page: Playwright Page ê°ì²´
            store_name: ìŒì‹ì ëª…
            store_address: ì£¼ì†Œ
            
        Returns:
            Tuple or None: (name, address, phone, hours, image, sub_category, tags)
        """
        # 1ì°¨ ì‹œë„: ~ë¡œ/~ê¸¸ + ë§¤ì¥ëª…
        if store_address:
            road_name = self._extract_road_name(store_address)
            if road_name:
                first_keyword = f"{road_name} {store_name}"
                logger.info(f"  1ì°¨ ê²€ìƒ‰: {first_keyword}")
                result = await self._search_single(page, first_keyword)
                if result:
                    return result
                await asyncio.sleep(4)
                logger.warning(f"  1ì°¨ ê²€ìƒ‰ ì‹¤íŒ¨")
        
        # 2ì°¨ ì‹œë„: ì „ì²´ ì£¼ì†Œ + ë§¤ì¥ëª…
        if store_address:
            second_keyword = f"{store_address} {store_name}"
            logger.info(f"  2ì°¨ ê²€ìƒ‰: {second_keyword}")
            result = await self._search_single(page, second_keyword)
            if result:
                return result
            await asyncio.sleep(4)
            logger.warning(f"  2ì°¨ ê²€ìƒ‰ ì‹¤íŒ¨")
        
        # 3ì°¨ ì‹œë„: ë§¤ì¥ëª…ë§Œ
        logger.info(f"  3ì°¨ ê²€ìƒ‰: {store_name}")
        result = await self._search_single(page, store_name)
        if result:
            return result
        await asyncio.sleep(4)
        logger.warning(f"  3ì°¨ ê²€ìƒ‰ ì‹¤íŒ¨")
        
        # 4ì°¨ ì‹œë„: ì£¼ì†Œë§Œ
        if store_address:
            logger.info(f"  4ì°¨ ê²€ìƒ‰: {store_address}")
            result = await self._search_single(page, store_address)
            if result:
                return result
            await asyncio.sleep(4)
            logger.warning(f"  4ì°¨ ê²€ìƒ‰ ì‹¤íŒ¨")
        
        logger.error(f"  ëª¨ë“  ê²€ìƒ‰ ì‹œë„ ì‹¤íŒ¨: {store_name}")
        return None
    
    async def _search_single(self, page: Page, keyword: str):
        """ë‹¨ì¼ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰"""
        try:
            # ë„¤ì´ë²„ ì§€ë„ ì´ë™
            await page.goto(self.naver_map_url)
            
            # ê²€ìƒ‰
            search_input_selector = '.input_search'
            await page.wait_for_selector(search_input_selector)
            await asyncio.sleep(1)
            
            await page.fill(search_input_selector, '')
            await asyncio.sleep(0.5)
            
            await page.fill(search_input_selector, keyword)
            await page.press(search_input_selector, 'Enter')
            
            # entry iframe ëŒ€ê¸°
            await page.wait_for_selector('iframe#entryIframe', timeout=10000)
            entry_frame = page.frame_locator('iframe#entryIframe')
            await asyncio.sleep(3)
            
            # ì •ë³´ ì¶”ì¶œ
            extractor = StoreDetailExtractor(entry_frame, page)
            return await extractor.extract_all_details()
            
        except TimeoutError:
            logger.error(f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        except Exception as e:
            logger.error(f"'{keyword}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return None


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # ========================================
    # í¬ë¡¤ë§ ì„¤ì •
    # ========================================
    headless_mode = False   # Trueë¡œ ì„¤ì •í•˜ë©´ ë¸Œë¼ìš°ì €ê°€ ë³´ì´ì§€ ì•ŠìŒ
    page_delay = 5          # Bluer í˜ì´ì§€ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    naver_delay = 30        # ë„¤ì´ë²„ í¬ë¡¤ë§ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    
    # ========================================
    # í¬ë¡¤ë§ ì‹¤í–‰
    # ========================================
    
    logger.info("=" * 80)
    logger.info("Bluer ìŒì‹ì  í¬ë¡¤ë§ ì‹œì‘")
    logger.info("=" * 80)
    
    try:
        crawler = BluerRestaurantCrawler(headless=headless_mode)
        await crawler.crawl_all_pages(delay=page_delay, naver_delay=naver_delay)
        
        logger.info("")
        logger.info("=" * 80)
        logger.info("âœ“ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())


if __name__ == "__main__":
    asyncio.run(main())