import json
import asyncio
from playwright.async_api import async_playwright, TimeoutError, Page
import re
from typing import Optional, List, Tuple
import sys, os
import datetime
import aiohttp
from dotenv import load_dotenv
import xml.etree.ElementTree as ET

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path="src/.env")

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from src.logger.custom_logger import get_logger


# ì™¸ë¶€ API ì„œë¹„ìŠ¤ import
from src.infra.external.seoul_district_api_service import SeoulDistrictAPIService
from src.infra.external.kakao_geocoding_service import GeocodingService
from src.infra.external.category_classifier_service import CategoryTypeClassifier

# ìœ í‹¸ë¦¬í‹° import
from src.service.crawl.utils.address_parser import AddressParser
from src.service.crawl.utils.store_detail_extractor import StoreDetailExtractor
from src.service.crawl.utils.store_data_saver import StoreDataSaver
from src.service.crawl.utils.search_strategy import NaverMapSearchStrategy
from src.service.crawl.utils.crawling_manager import CrawlingManager



class NaverMapDistrictCrawler:
    """ì„œìš¸ì‹œ ê° êµ¬ API ë°ì´í„° í¬ë¡¤ë§ í´ë˜ìŠ¤"""
    
    def __init__(self, district_name: str, headless: bool = False):
        self.district_name = district_name
        self.headless = headless
        self.naver_map_url = "https://map.naver.com/v5/search"
        self.geocoding_service = GeocodingService()
        self.category_classifier = CategoryTypeClassifier()
        self.data_saver = StoreDataSaver()
        self.search_strategy = NaverMapSearchStrategy()
        self.crawling_manager = CrawlingManager(district_name)
        self.logger = get_logger(__name__)
    
    async def crawl_district_api(self, delay: int = 20):
        """í•´ë‹¹ êµ¬ì˜ APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ í¬ë¡¤ë§"""
        # API ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        api_service = SeoulDistrictAPIService(self.district_name)
        api_data = await api_service.fetch_all_restaurants()
        
        if not api_data:
            self.logger.warning(f"{self.district_name} APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        stores = api_service.convert_to_store_format(api_data)
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=self.headless,
                args=['--enable-features=ClipboardAPI']
            )
            context = await browser.new_context(
                permissions=['clipboard-read', 'clipboard-write']
            )
            page = await context.new_page()
            
            try:
                # í¬ë¡¤ë§ ë§¤ë‹ˆì €ë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
                await self.crawling_manager.execute_crawling_with_save(
                    stores=stores,
                    crawl_func=lambda store, idx, total: self._crawl_single_store(page, store),
                    save_func=self._save_wrapper,
                    delay=delay
                )
                
            except Exception as e:
                self.logger.error(f"{self.district_name} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
            finally:
                await context.close()
                await browser.close()
    
    async def _crawl_single_store(self, page: Page, store: dict):
        """ë‹¨ì¼ ë§¤ì¥ í¬ë¡¤ë§"""
        store_name = store['name']
        store_address = store['address']
        road_address = store['road_address']
        
        # ê²€ìƒ‰ ì „ëµ ì‚¬ìš©
        async def extract_callback(entry_frame, page):
            extractor = StoreDetailExtractor(entry_frame, page)
            return await extractor.extract_all_details()
        
        return await self.search_strategy.search_with_multiple_strategies(
            page=page,
            store_name=store_name,
            store_address=store_address,
            road_address=road_address,
            extractor_callback=extract_callback
        )
    
    async def _save_wrapper(self, idx: int, total: int, store_data: tuple, store_name: str):
        """ì €ì¥ ë˜í¼"""
        return await self.data_saver.save_store_data(
            idx=idx,
            total=total,
            store_data=store_data,
            store_name=store_name,
            log_prefix=self.district_name
        )


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger = get_logger(__name__)
    # ========================================
    # ğŸ”§ ì—¬ê¸°ì„œ í¬ë¡¤ë§í•  êµ¬ë¥¼ ì„ íƒí•˜ì„¸ìš”!
    # ========================================
    
    # ë‹¨ì¼ êµ¬ í¬ë¡¤ë§ ì˜ˆì‹œ:
    # district_name = 'ê°•ë‚¨êµ¬'
    # district_name = 'ì„œì´ˆêµ¬'
    # district_name = 'ë§ˆí¬êµ¬'
    
    # ë˜ëŠ” ì—¬ëŸ¬ êµ¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§:
    districts_to_crawl = [
        'ê°•ë‚¨êµ¬',
        'ê°•ë™êµ¬',
        'ê°•ë¶êµ¬',
        'ê°•ì„œêµ¬',
        'ê´€ì•…êµ¬',
        'ê´‘ì§„êµ¬',
        'êµ¬ë¡œêµ¬',
        'ê¸ˆì²œêµ¬',
        'ë…¸ì›êµ¬',
        'ë„ë´‰êµ¬',
        'ë™ëŒ€ë¬¸êµ¬',
        'ë™ì‘êµ¬',
        'ë§ˆí¬êµ¬',
        'ì„œëŒ€ë¬¸êµ¬',
        'ì„œì´ˆêµ¬',
        'ì„±ë™êµ¬',
        'ì„±ë¶êµ¬',
        'ì†¡íŒŒêµ¬',
        'ì–‘ì²œêµ¬',
        'ì˜ë“±í¬êµ¬',
        'ìš©ì‚°êµ¬',
        'ì€í‰êµ¬',
        'ì¢…ë¡œêµ¬',
        'ì¤‘êµ¬',
        'ì¤‘ë‘êµ¬'
    ]
    
    # ========================================
    # í¬ë¡¤ë§ ì„¤ì •
    # ========================================
    headless_mode = False  # Trueë¡œ ì„¤ì •í•˜ë©´ ë¸Œë¼ìš°ì €ê°€ ë³´ì´ì§€ ì•ŠìŒ
    delay_seconds = 30     # í¬ë¡¤ë§ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    
    # ========================================
    # í¬ë¡¤ë§ ì‹¤í–‰
    # ========================================
    
    # logger.info("=" * 80)
    # logger.info(f"í¬ë¡¤ë§ ì‹œì‘ - ì´ {len(districts_to_crawl)}ê°œ êµ¬")
    # logger.info(f"ëŒ€ìƒ êµ¬: {', '.join(districts_to_crawl)}")
    # logger.info("=" * 80)
    
    for idx, district_name in enumerate(districts_to_crawl, 1):
        try:
            # logger.info("")
            # logger.info("=" * 80)
            logger.info(f"[{idx}/{len(districts_to_crawl)}] {district_name} í¬ë¡¤ë§ ì‹œì‘")
            # logger.info("=" * 80)
            
            # í¬ë¡¤ëŸ¬ ìƒì„±
            crawler = NaverMapDistrictCrawler(
                district_name=district_name,
                headless=headless_mode
            )
            
            # í•´ë‹¹ êµ¬ì˜ API ë°ì´í„°ë¡œ í¬ë¡¤ë§ ì‹œì‘
            await crawler.crawl_district_api(delay=delay_seconds)
            
            # logger.info("")
            # logger.info("=" * 80)
            logger.info(f"[{idx}/{len(districts_to_crawl)}] {district_name} í¬ë¡¤ë§ ì™„ë£Œ!")
            # logger.info("=" * 80)
            
            # ë‹¤ìŒ êµ¬ë¡œ ë„˜ì–´ê°€ê¸° ì „ ëŒ€ê¸° (ë§ˆì§€ë§‰ êµ¬ê°€ ì•„ë‹Œ ê²½ìš°)
            if idx < len(districts_to_crawl):
                wait_time = 60  # êµ¬ ì‚¬ì´ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
                # logger.info(f"ë‹¤ìŒ êµ¬ í¬ë¡¤ë§ ì „ {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...")
                await asyncio.sleep(wait_time)
                
        except Exception as e:
            logger.error(f"{district_name} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ë‹¤ìŒ êµ¬ ì§„í–‰ ì—¬ë¶€ í™•ì¸
            if idx < len(districts_to_crawl):
                # logger.info(f"ë‹¤ìŒ êµ¬({districts_to_crawl[idx]})ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
                await asyncio.sleep(30)
    
    # logger.info("")
    # logger.info("=" * 80)
    logger.info("ëª¨ë“  êµ¬ í¬ë¡¤ë§ ì™„ë£Œ!")
    # logger.info("=" * 80)