import json
import asyncio
from playwright.async_api import async_playwright, TimeoutError, Page
import re
from typing import Optional, List, Tuple
import sys, os
import datetime
import aiohttp
from dotenv import load_dotenv
import xml.etree.ElementTree as ET

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path="src/.env")

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from src.logger.logger_handler import get_logger
from src.domain.dto.insert_category_dto import InsertCategoryDto
from src.domain.dto.insert_category_tags_dto import InsertCategoryTagsDTO
from src.service.crawl.insert_crawled import insert_category, insert_category_tags, insert_tags
from src.service.crawl.update_crawled import update_category, update_category_tags
from src.infra.database.repository.category_repository import CategoryRepository
from src.infra.database.repository.category_tags_repository import CategoryTagsRepository
from src.service.crawl.store_detail_extractor import StoreDetailExtractor
from src.service.crawl.store_data_saver import StoreDataSaver

# ì™¸ë¶€ API ì„œë¹„ìŠ¤ import
from src.infra.external.seoul_district_api_service import SeoulDistrictAPIService
from src.infra.external.kakao_geocoding_service import GeocodingService
from src.infra.external.category_classifier_service import CategoryTypeClassifier

# ìœ í‹¸ë¦¬í‹° import
from src.service.crawl.utils.address_parser import AddressParser

# ë¡œê±° ì´ˆê¸°í™”
logger = get_logger('crawling_naver_model')

class NaverMapDistrictCrawler:
    """ì„œìš¸ì‹œ ê° êµ¬ API ë°ì´í„° í¬ë¡¤ë§ í´ë˜ìŠ¤"""
    
    def __init__(self, district_name: str, headless: bool = False):
        """
        Args:
            district_name: í¬ë¡¤ë§í•  êµ¬ ì´ë¦„ (ì˜ˆ: 'ê°•ë‚¨êµ¬', 'ì„œì´ˆêµ¬')
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
        """
        self.district_name = district_name
        self.headless = headless
        self.naver_map_url = "https://map.naver.com/v5/search"
        self.geocoding_service = GeocodingService()
        self.category_classifier = CategoryTypeClassifier()
        
        self.data_saver = StoreDataSaver()
        
        # logger.info(f"âœ“ {district_name} í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def _save_store_data(self, idx: int, total: int, store_data: Tuple, store_name: str, store_id: int, api_sub_category: str):
        """ê³µí†µ ì €ì¥ ë¡œì§ í˜¸ì¶œ"""
        return await self.data_saver.save_store_data(
            idx=idx,
            total=total,
            store_data=store_data,
            store_name=store_name,
            log_prefix=self.district_name
        )
    
    async def crawl_district_api(self, delay: int = 20):
        """
        í•´ë‹¹ êµ¬ì˜ APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ í¬ë¡¤ë§
        í¬ë¡¤ë§ê³¼ ì €ì¥ì„ ë¶„ë¦¬í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
        
        Args:
            delay: í¬ë¡¤ë§ ê°„ ë”œë ˆì´ (ì´ˆ)
        """
        # í•´ë‹¹ êµ¬ì˜ APIì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸°)
        api_service = SeoulDistrictAPIService(self.district_name)
        api_data = await api_service.fetch_all_restaurants()
        
        if not api_data:
            logger.warning(f"{self.district_name} APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # í¬ë¡¤ë§ìš© í¬ë§·ìœ¼ë¡œ ë³€í™˜
        stores = api_service.convert_to_store_format(api_data)
        
        total = len(stores)
        success_count = 0
        fail_count = 0
        
        logger.info(f"ì´ {total}ê°œ {self.district_name} ëª¨ë²”ìŒì‹ì  í¬ë¡¤ë§ ì‹œì‘")
        # logger.info("=" * 60)
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=self.headless,
                args=['--enable-features=ClipboardAPI']  # í´ë¦½ë³´ë“œ API í™œì„±í™”
            )
            
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹œ í´ë¦½ë³´ë“œ ê¶Œí•œ ë¶€ì—¬
            context = await browser.new_context(
                permissions=['clipboard-read', 'clipboard-write']
            )
            page = await context.new_page()
            
            try:
                # ì €ì¥ íƒœìŠ¤í¬ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
                save_tasks = []
                
                for idx, store in enumerate(stores, 1):
                    store_id = store['id']
                    store_name = store['name']
                    store_address = store['address']  # ì§€ë²ˆ ì£¼ì†Œ
                    road_address = store['road_address']  # ë„ë¡œëª… ì£¼ì†Œ (SITE_ADDR_RD)
                    api_sub_category = store['sub_category']  # API ì„œë¸Œ ì¹´í…Œê³ ë¦¬
                    admdng_nm = store['admdng_nm']
                    
                    logger.info(f"[{self.district_name} í¬ë¡¤ë§ {idx}/{total}] ID {store_id}: '{store_name}' (í–‰ì •ë™: {admdng_nm}) í¬ë¡¤ë§ ì§„í–‰ ì¤‘...")
                    # logger.info(f"  - API ì„œë¸Œ ì¹´í…Œê³ ë¦¬: {api_sub_category}")
                    # logger.info(f"  - ì§€ë²ˆ ì£¼ì†Œ: {store_address}")
                    # logger.info(f"  - ë„ë¡œëª… ì£¼ì†Œ: {road_address}")
                    
                    # ë„¤ì´ë²„ ì§€ë„ì—ì„œ ê²€ìƒ‰ (ë„ë¡œëª… ì£¼ì†Œ ì „ë‹¬)
                    store_data = await self._search_and_extract(page, store_name, store_address, road_address)
                    
                    if store_data:
                        # store_dataì—ì„œ ë„¤ì´ë²„ ì„œë¸Œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                        naver_sub_category = store_data[5]  # (name, address, phone, hours, image, sub_category, tags)
                        # logger.info(f"  - ë„¤ì´ë²„ ì„œë¸Œ ì¹´í…Œê³ ë¦¬: {naver_sub_category}")
                        logger.info(f"[{self.district_name} í¬ë¡¤ë§ {idx}/{total}] ID {store_id} '{store_name}' í¬ë¡¤ë§ ì™„ë£Œ")
                        
                        # ì €ì¥ íƒœìŠ¤í¬ ìƒì„± (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)
                        save_task = asyncio.create_task(
                            self._save_store_data(idx, total, store_data, store_name, store_id, api_sub_category)
                        )
                        save_tasks.append(save_task)
                        
                        # ë§ˆì§€ë§‰ ìƒì ì´ ì•„ë‹ˆë©´ ë”œë ˆì´
                        if idx < total:
                            # logger.info(f"[{self.district_name} ëŒ€ê¸°] {delay}ì´ˆ ëŒ€ê¸° ì¤‘... (ì €ì¥ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰)")
                            await asyncio.sleep(delay)
                    else:
                        fail_count += 1
                        logger.error(f"[{self.district_name} í¬ë¡¤ë§ {idx}/{total}] ID {store_id} '{store_name}' í¬ë¡¤ë§ ì‹¤íŒ¨")
                        
                        # ì‹¤íŒ¨í•´ë„ ë”œë ˆì´
                        if idx < total:
                            # logger.info(f"[{self.district_name} ëŒ€ê¸°] {delay}ì´ˆ ëŒ€ê¸° ì¤‘...")
                            await asyncio.sleep(delay)
                
                # ëª¨ë“  í¬ë¡¤ë§ì´ ëë‚œ í›„ ì €ì¥ íƒœìŠ¤í¬ë“¤ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                # logger.info("=" * 60)
                logger.info(f"{self.district_name} ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! ì €ì¥ ì‘ì—… ì™„ë£Œ ëŒ€ê¸° ì¤‘... ({len(save_tasks)}ê°œ)")
                # logger.info("=" * 60)
                
                if save_tasks:
                    save_results = await asyncio.gather(*save_tasks, return_exceptions=True)
                    
                    # ì €ì¥ ê²°ê³¼ ì§‘ê³„
                    for result in save_results:
                        if isinstance(result, Exception):
                            fail_count += 1
                        elif isinstance(result, tuple):
                            success, msg = result
                            if success:
                                success_count += 1
                            else:
                                fail_count += 1
                
                # logger.info("=" * 60)
                logger.info(f"{self.district_name} ì „ì²´ ì‘ì—… ì™„ë£Œ: ì„±ê³µ {success_count}/{total}, ì‹¤íŒ¨ {fail_count}/{total}")
                # logger.info("=" * 60)
                
            except Exception as e:
                logger.error(f"{self.district_name} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                logger.error(traceback.format_exc())
            finally:
                await context.close()
                await browser.close()

    async def _search_and_extract(self, page: Page, store_name: str, store_address: str, road_address: str = ""):
        """ë„¤ì´ë²„ ì§€ë„ì—ì„œ ê²€ìƒ‰ ë° ì •ë³´ ì¶”ì¶œ (ë„ë¡œëª… ì£¼ì†Œ ìš°ì„ )"""
        
        # ë„ë¡œëª… ì£¼ì†Œê°€ ìˆëŠ” ê²½ìš° ìš°ì„  ê²€ìƒ‰
        if road_address and road_address.strip():
            # 1ì°¨ ì‹œë„: ë„ë¡œëª… ì£¼ì†Œ(~ë¡œ/ê¸¸ê¹Œì§€) + ë§¤ì¥ëª…
            road_parts = road_address.split()
            if len(road_parts) >= 2:
                # ~ë¡œ, ~ê¸¸ê¹Œì§€ë§Œ ì¶”ì¶œ
                road_keyword = self._extract_road_name(road_parts)
                if road_keyword:
                    first_keyword = f"{road_keyword} {store_name}"
                    # logger.info(f"1ì°¨ ê²€ìƒ‰: {first_keyword}")
                    result = await self._search_single(page, first_keyword)
                    if result:
                        return result
                    
                    await asyncio.sleep(4)
                    logger.warning(f"1ì°¨ ê²€ìƒ‰ ì‹¤íŒ¨")
            
            # 2ì°¨ ì‹œë„: ë„ë¡œëª… ì „ì²´ ì£¼ì†Œ + ë§¤ì¥ëª…
            second_keyword = f"{road_address} {store_name}"
            # logger.info(f"2ì°¨ ê²€ìƒ‰: {second_keyword}")
            result = await self._search_single(page, second_keyword)
            if result:
                return result
            
            await asyncio.sleep(4)
            logger.warning(f"2ì°¨ ê²€ìƒ‰ ì‹¤íŒ¨")
        
        # 3ì°¨ ì‹œë„: ì§€ë²ˆì£¼ì†Œ(~ë™ê¹Œì§€) + ê°€ê²Œëª…
        address_parts = store_address.split()
        if len(address_parts) >= 3:
            third_keyword = f"{self._extract_search_address(address_parts)} {store_name}"
        else:
            third_keyword = f"{store_address} {store_name}"
        
        # logger.info(f"3ì°¨ ê²€ìƒ‰: {third_keyword}")
        result = await self._search_single(page, third_keyword)
        if result:
            return result
        
        await asyncio.sleep(4)
        logger.warning(f"3ì°¨ ê²€ìƒ‰ ì‹¤íŒ¨")
        
        # 4ì°¨ ì‹œë„: ë§¤ì¥ëª…ë§Œ
        # logger.info(f"4ì°¨ ê²€ìƒ‰: {store_name}")
        result = await self._search_single(page, store_name)
        if result:
            return result
        
        await asyncio.sleep(4)
        logger.warning(f"4ì°¨ ê²€ìƒ‰ ì‹¤íŒ¨")
        
        # 5ì°¨ ì‹œë„: ì§€ë²ˆ ì£¼ì†Œë§Œ
        # logger.info(f"5ì°¨ ê²€ìƒ‰: {store_address}")
        result = await self._search_single(page, store_address)
        if result:
            return result
        
        await asyncio.sleep(4)
        logger.warning(f"5ì°¨ ê²€ìƒ‰ ì‹¤íŒ¨")
        
        # 6ì°¨ ì‹œë„: ì§€ë²ˆ ì „ì²´ ì£¼ì†Œ + ë§¤ì¥ëª…
        sixth_keyword = f"{store_address} {store_name}"
        # logger.info(f"6ì°¨ ê²€ìƒ‰: {sixth_keyword}")
        result = await self._search_single(page, sixth_keyword)
        if result:
            return result
        
        logger.error(f"ëª¨ë“  ê²€ìƒ‰ ì‹œë„ ì‹¤íŒ¨: {store_name}")
        return None

    def _extract_road_name(self, road_parts: List[str]) -> str:
        """ë„ë¡œëª… ì£¼ì†Œì—ì„œ ~ë¡œ, ~ê¸¸ê¹Œì§€ë§Œ ì¶”ì¶œ"""
        if not road_parts:
            return ""
        
        result_parts = []
        
        for part in road_parts:
            result_parts.append(part)
            
            # ~ë¡œ, ~ê¸¸ì´ ë‚˜ì˜¤ë©´ ë°”ë¡œ ì¢…ë£Œ
            if part.endswith('ë¡œ') or part.endswith('ê¸¸'):
                break
            
            # ì•ˆì „ì¥ì¹˜: ìµœëŒ€ 4ê°œ ìš”ì†Œê¹Œì§€
            if len(result_parts) >= 4:
                break
        
        return " ".join(result_parts)
    
    def _extract_search_address(self, address_parts: List[str]) -> str:
        """ì£¼ì†Œì—ì„œ ê²€ìƒ‰ì— ì í•©í•œ ë¶€ë¶„ ì¶”ì¶œ (ì§€ë²ˆ ì£¼ì†Œ ~ë™ê¹Œì§€)"""
        if not address_parts:
            return ""
        
        result_parts = []
        
        for part in address_parts:
            result_parts.append(part)
            
            # ì/ë©´/ë™ì´ ë‚˜ì˜¤ë©´ ë°”ë¡œ ì¢…ë£Œ
            if part.endswith('ì') or part.endswith('ë©´') or part.endswith('ë™') or part.endswith('ë¦¬'):
                break
            
            # ë„ë¡œëª…(~ë¡œ, ~ê¸¸)ì´ ë‚˜ì˜¤ë©´ ì¢…ë£Œ
            elif part.endswith('ë¡œ') or part.endswith('ê¸¸'):
                break
            
            # ì•ˆì „ì¥ì¹˜: ìµœëŒ€ 4ê°œ ìš”ì†Œê¹Œì§€
            if len(result_parts) >= 4:
                break
        
        return " ".join(result_parts)
    
    async def _search_single(self, page: Page, keyword: str):
        """ë‹¨ì¼ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰"""
        try:
            # ë„¤ì´ë²„ ì§€ë„ ì´ë™
            await page.goto(self.naver_map_url)
            
            # ê²€ìƒ‰
            search_input_selector = '.input_search'
            await page.wait_for_selector(search_input_selector)
            await asyncio.sleep(1)
            
            await page.fill(search_input_selector, '')
            await asyncio.sleep(0.5)
            
            await page.fill(search_input_selector, keyword)
            await page.press(search_input_selector, 'Enter')
            
            # entry iframe ëŒ€ê¸°
            await page.wait_for_selector('iframe#entryIframe', timeout=10000)
            entry_frame = page.frame_locator('iframe#entryIframe')
            await asyncio.sleep(3)
            
            # ì •ë³´ ì¶”ì¶œ
            extractor = StoreDetailExtractor(entry_frame, page)
            return await extractor.extract_all_details()
            
        except TimeoutError:
            logger.error(f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        except Exception as e:
            logger.error(f"'{keyword}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return None


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # ========================================
    # ğŸ”§ ì—¬ê¸°ì„œ í¬ë¡¤ë§í•  êµ¬ë¥¼ ì„ íƒí•˜ì„¸ìš”!
    # ========================================
    
    # ë‹¨ì¼ êµ¬ í¬ë¡¤ë§ ì˜ˆì‹œ:
    # district_name = 'ê°•ë‚¨êµ¬'
    # district_name = 'ì„œì´ˆêµ¬'
    # district_name = 'ë§ˆí¬êµ¬'
    
    # ë˜ëŠ” ì—¬ëŸ¬ êµ¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§:
    districts_to_crawl = [
        'ê°•ë‚¨êµ¬',
        'ê°•ë™êµ¬',
        'ê°•ë¶êµ¬',
        'ê°•ì„œêµ¬',
        'ê´€ì•…êµ¬',
        'ê´‘ì§„êµ¬',
        'êµ¬ë¡œêµ¬',
        'ê¸ˆì²œêµ¬',
        'ë…¸ì›êµ¬',
        'ë„ë´‰êµ¬',
        'ë™ëŒ€ë¬¸êµ¬',
        'ë™ì‘êµ¬',
        'ë§ˆí¬êµ¬',
        'ì„œëŒ€ë¬¸êµ¬',
        'ì„œì´ˆêµ¬',
        'ì„±ë™êµ¬',
        'ì„±ë¶êµ¬',
        'ì†¡íŒŒêµ¬',
        'ì–‘ì²œêµ¬',
        'ì˜ë“±í¬êµ¬',
        'ìš©ì‚°êµ¬',
        'ì€í‰êµ¬',
        'ì¢…ë¡œêµ¬',
        'ì¤‘êµ¬',
        'ì¤‘ë‘êµ¬'
    ]
    
    # ========================================
    # í¬ë¡¤ë§ ì„¤ì •
    # ========================================
    headless_mode = False  # Trueë¡œ ì„¤ì •í•˜ë©´ ë¸Œë¼ìš°ì €ê°€ ë³´ì´ì§€ ì•ŠìŒ
    delay_seconds = 30     # í¬ë¡¤ë§ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    
    # ========================================
    # í¬ë¡¤ë§ ì‹¤í–‰
    # ========================================
    
    # logger.info("=" * 80)
    # logger.info(f"í¬ë¡¤ë§ ì‹œì‘ - ì´ {len(districts_to_crawl)}ê°œ êµ¬")
    # logger.info(f"ëŒ€ìƒ êµ¬: {', '.join(districts_to_crawl)}")
    # logger.info("=" * 80)
    
    for idx, district_name in enumerate(districts_to_crawl, 1):
        try:
            # logger.info("")
            # logger.info("=" * 80)
            logger.info(f"[{idx}/{len(districts_to_crawl)}] {district_name} í¬ë¡¤ë§ ì‹œì‘")
            # logger.info("=" * 80)
            
            # í¬ë¡¤ëŸ¬ ìƒì„±
            crawler = NaverMapDistrictCrawler(
                district_name=district_name,
                headless=headless_mode
            )
            
            # í•´ë‹¹ êµ¬ì˜ API ë°ì´í„°ë¡œ í¬ë¡¤ë§ ì‹œì‘
            await crawler.crawl_district_api(delay=delay_seconds)
            
            # logger.info("")
            # logger.info("=" * 80)
            logger.info(f"[{idx}/{len(districts_to_crawl)}] {district_name} í¬ë¡¤ë§ ì™„ë£Œ!")
            # logger.info("=" * 80)
            
            # ë‹¤ìŒ êµ¬ë¡œ ë„˜ì–´ê°€ê¸° ì „ ëŒ€ê¸° (ë§ˆì§€ë§‰ êµ¬ê°€ ì•„ë‹Œ ê²½ìš°)
            if idx < len(districts_to_crawl):
                wait_time = 60  # êµ¬ ì‚¬ì´ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
                # logger.info(f"ë‹¤ìŒ êµ¬ í¬ë¡¤ë§ ì „ {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...")
                await asyncio.sleep(wait_time)
                
        except Exception as e:
            logger.error(f"{district_name} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ë‹¤ìŒ êµ¬ ì§„í–‰ ì—¬ë¶€ í™•ì¸
            if idx < len(districts_to_crawl):
                # logger.info(f"ë‹¤ìŒ êµ¬({districts_to_crawl[idx]})ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
                await asyncio.sleep(30)
    
    # logger.info("")
    # logger.info("=" * 80)
    logger.info("ëª¨ë“  êµ¬ í¬ë¡¤ë§ ì™„ë£Œ!")
    # logger.info("=" * 80)


if __name__ == "__main__":
    asyncio.run(main())