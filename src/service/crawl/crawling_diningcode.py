"""
DiningCode ì›¹ì‚¬ì´íŠ¸ ìŒì‹ì  í¬ë¡¤ë§ ëª¨ë“ˆ (ë©”ëª¨ë¦¬ ìµœì í™” + ë´‡ ìš°íšŒ + ë³‘ë ¬ ì²˜ë¦¬)
1ë‹¨ê³„: DiningCodeì—ì„œ ì „ì²´ ëª©ë¡ ìˆ˜ì§‘ â†’ 2ë‹¨ê³„: ë„¤ì´ë²„ ì§€ë„ì—ì„œ ë°°ì¹˜ ë³‘ë ¬ í¬ë¡¤ë§
"""
import asyncio
import re

from playwright.async_api import async_playwright, TimeoutError, Page

from src.logger.custom_logger import get_logger
from src.service.crawl.utils.crawling_manager import CrawlingManager
from src.service.crawl.utils.human_like_actions import HumanLikeActions
# ê³µí†µ ëª¨ë“ˆ import
from src.service.crawl.utils.optimized_browser_manager import OptimizedBrowserManager
from src.service.crawl.utils.search_strategy import NaverMapSearchStrategy
from src.service.crawl.utils.store_data_saver import StoreDataSaver
from src.service.crawl.utils.store_detail_extractor import StoreDetailExtractor


class DiningCodeRestaurantCrawler:
    """DiningCode ì›¹ì‚¬ì´íŠ¸ ìŒì‹ì  í¬ë¡¤ë§ í´ë˜ìŠ¤ (ë³‘ë ¬ ì²˜ë¦¬)"""
    
    RESTART_INTERVAL = 50  # 50ê°œë§ˆë‹¤ ì»¨í…ìŠ¤íŠ¸ ì¬ì‹œì‘
    
    def __init__(self, headless: bool = False):
        self.logger = get_logger(__name__)
        self.headless = headless
        self.diningcode_url = "https://www.diningcode.com/list.dc?query=%EC%84%9C%EC%9A%B8%20%EC%B9%B4%ED%8E%98"
        self.data_saver = StoreDataSaver()
        self.search_strategy = NaverMapSearchStrategy()
        self.human_actions = HumanLikeActions()
        self.success_count = 0
        self.fail_count = 0
    
    async def crawl_all_pages(self, delay: int = 5, naver_delay: int = 20):
        """
        DiningCode ì „ì²´ í˜ì´ì§€ ë³‘ë ¬ í¬ë¡¤ë§
        1ë‹¨ê³„: DiningCodeì—ì„œ ì „ì²´ ëª©ë¡ ìˆ˜ì§‘ â†’ 2ë‹¨ê³„: ë„¤ì´ë²„ ì§€ë„ì—ì„œ ë°°ì¹˜ ë³‘ë ¬ í¬ë¡¤ë§
        
        Args:
            delay: DiningCode í˜ì´ì§€ ê°„ ë”œë ˆì´ (ì´ˆ)
            naver_delay: ë„¤ì´ë²„ ì§€ë„ í¬ë¡¤ë§ ë”œë ˆì´ (ì´ˆ)
        """
        async with async_playwright() as p:
            # 1ë‹¨ê³„: DiningCodeì—ì„œ ì „ì²´ ìŒì‹ì  ëª©ë¡ ìˆ˜ì§‘
            self.logger.info("1ë‹¨ê³„: DiningCode ì „ì²´ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘")
            
            all_restaurants = await self._collect_all_restaurants(p, delay)
            
            if not all_restaurants:
                self.logger.warning("ìˆ˜ì§‘ëœ ìŒì‹ì ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            total = len(all_restaurants)
            self.logger.info(f"ì´ {total}ê°œ ìŒì‹ì  ìˆ˜ì§‘ ì™„ë£Œ")
            
            # 2ë‹¨ê³„: ë„¤ì´ë²„ ì§€ë„ì—ì„œ ë°°ì¹˜ ë³‘ë ¬ í¬ë¡¤ë§
            self.logger.info("2ë‹¨ê³„: ë„¤ì´ë²„ ì§€ë„ ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘")
            self.logger.info(f"ë°°ì¹˜ í¬ê¸°: {self.RESTART_INTERVAL}ê°œ")
            self.logger.info(f"ì˜ˆìƒ ë°°ì¹˜ ìˆ˜: {(total + self.RESTART_INTERVAL - 1) // self.RESTART_INTERVAL}ê°œ")
            
            naver_browser = await OptimizedBrowserManager.create_optimized_browser(p, self.headless)
            
            try:
                for batch_start in range(0, total, self.RESTART_INTERVAL):
                    batch_end = min(batch_start + self.RESTART_INTERVAL, total)
                    batch = all_restaurants[batch_start:batch_end]
                    
                    batch_num = batch_start // self.RESTART_INTERVAL + 1
                    total_batches = (total + self.RESTART_INTERVAL - 1) // self.RESTART_INTERVAL
                    
                    self.logger.info(f"ë°°ì¹˜ {batch_num}/{total_batches}: {batch_start+1}~{batch_end}/{total}")
                    
                    # ìƒˆ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                    context = await OptimizedBrowserManager.create_stealth_context(naver_browser)
                    page = await context.new_page()
                    
                    try:
                        await self._process_batch_parallel(
                            page, batch, batch_start, total, naver_delay
                        )
                    except Exception as e:
                        self.logger.error(f"ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        import traceback
                        self.logger.error(traceback.format_exc())
                    finally:
                        await context.close()
                        await asyncio.sleep(3)
                        
                        # ë°°ì¹˜ ê°„ íœ´ì‹
                        if batch_end < total:
                            import random
                            rest_time = random.uniform(20, 40)
                            self.logger.info(f"ë°°ì¹˜ {batch_num} ì™„ë£Œ, {rest_time:.0f}ì´ˆ íœ´ì‹...\n")
                            await asyncio.sleep(rest_time)
                
                # ìµœì¢… ê²°ê³¼
                self.logger.info(f"ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")
                self.logger.info(f"ì´ ì²˜ë¦¬: {total}ê°œ")
                self.logger.info(f"ì„±ê³µ: {self.success_count}ê°œ")
                self.logger.info(f"ì‹¤íŒ¨: {self.fail_count}ê°œ")
                if total > 0:
                    self.logger.info(f"ì„±ê³µë¥ : {self.success_count/total*100:.1f}%")
                
            finally:
                await naver_browser.close()
    
    async def _collect_all_restaurants(self, playwright, delay: int) -> list:
        """DiningCodeì—ì„œ ì „ì²´ ìŒì‹ì  ëª©ë¡ë§Œ ìˆ˜ì§‘"""
        # ğŸ”¥ ë´‡ íƒì§€ íšŒí”¼ë¥¼ ìœ„í•œ ìµœì í™”ëœ ë¸Œë¼ìš°ì € ì‚¬ìš©
        browser = await OptimizedBrowserManager.create_optimized_browser(playwright, self.headless)
        context = await OptimizedBrowserManager.create_stealth_context(browser)
        page = await context.new_page()
        
        all_restaurants = []
        
        try:
            self.logger.info(f"DiningCode í˜ì´ì§€ ì ‘ì† ì¤‘...")
            
            # ğŸ”¥ íƒ€ì„ì•„ì›ƒ ì¦ê°€ + domcontentloadedë¡œ ë³€ê²½ (ë” ë¹ ë¥¸ ë¡œë”©)
            try:
                await page.goto(
                    self.diningcode_url, 
                    wait_until='domcontentloaded',  # networkidle ëŒ€ì‹  domcontentloaded
                    timeout=60000  # 60ì´ˆë¡œ ì¦ê°€
                )
                self.logger.info("í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
            except TimeoutError:
                self.logger.warning("í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ, í˜„ì¬ ìƒíƒœë¡œ ì§„í–‰ ì‹œë„...")
            
            # í˜ì´ì§€ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ì¶”ê°€ ëŒ€ê¸°
            await asyncio.sleep(5)
            
            # ğŸ”¥ í˜ì´ì§€ ìŠ¤í¬ë¡¤ (ì»¨í…ì¸  ë¡œë“œ ìœ ë„)
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight / 2)')
            await asyncio.sleep(2)
            await page.evaluate('window.scrollTo(0, 0)')
            await asyncio.sleep(2)
            
            # "ë§›ì§‘ ë”ë³´ê¸°" ë²„íŠ¼ì„ ì‚¬ë¼ì§ˆ ë•Œê¹Œì§€ í´ë¦­
            await self._click_load_more_button(page)
            
            # ìŒì‹ì  ëª©ë¡ ìˆ˜ì§‘
            self.logger.info("ìŒì‹ì  ëª©ë¡ ì¶”ì¶œ ì¤‘...")
            restaurants = await self._extract_restaurants_from_page(page)
            
            if restaurants:
                self.logger.info(f"ì´ {len(restaurants)}ê°œ ìŒì‹ì  ìˆ˜ì§‘")
                all_restaurants.extend(restaurants)
            else:
                self.logger.warning("ìŒì‹ì ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            self.logger.error(f"DiningCode ëª©ë¡ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
        finally:
            await context.close()
            await browser.close()
        
        return all_restaurants
    
    async def _click_load_more_button(self, page: Page):
        """
        "ë§›ì§‘ ë”ë³´ê¸°" ë²„íŠ¼ì„ ì‚¬ë¼ì§ˆ ë•Œê¹Œì§€ ë°˜ë³µ í´ë¦­
        
        Args:
            page: Playwright Page ê°ì²´
        """
        click_count = 0
        max_attempts = 100  # ë¬´í•œ ë£¨í”„ ë°©ì§€
        
        self.logger.info("'ë§›ì§‘ ë”ë³´ê¸°' ë²„íŠ¼ í´ë¦­ ì‹œì‘...")
        
        while click_count < max_attempts:
            try:
                # ë”ë³´ê¸° ë²„íŠ¼ ì„ íƒì (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
                selectors = [
                    'div.SearchMore.upper[aria-label="search more in here"]',
                    'div[aria-label="search more in here"]'
                ]
                
                button_found = False
                load_more_button = None
                
                # ì—¬ëŸ¬ ì„ íƒì ì‹œë„
                for selector in selectors:
                    try:
                        load_more_button = page.locator(selector)
                        if await load_more_button.count() > 0:
                            button_found = True
                            self.logger.debug(f"ë²„íŠ¼ ë°œê²¬: {selector}")
                            break
                    except:
                        continue
                
                if not button_found or load_more_button is None:
                    self.logger.info(f"'ë§›ì§‘ ë”ë³´ê¸°' ë²„íŠ¼ì„ ë” ì´ìƒ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì´ {click_count}íšŒ í´ë¦­)")
                    break
                
                # ë²„íŠ¼ì´ ë³´ì´ëŠ”ì§€ í™•ì¸ (íƒ€ì„ì•„ì›ƒ 10ì´ˆ)
                if await load_more_button.is_visible(timeout=10000):
                    # ë²„íŠ¼ìœ¼ë¡œ ìŠ¤í¬ë¡¤
                    await load_more_button.scroll_into_view_if_needed()
                    await asyncio.sleep(1)
                    
                    # ğŸ”¥ ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ í´ë¦­ ì‹œë„
                    try:
                        # 1. ì¼ë°˜ í´ë¦­
                        await load_more_button.click(timeout=5000)
                    except:
                        try:
                            # 2. force í´ë¦­
                            await load_more_button.click(force=True, timeout=5000)
                        except:
                            # 3. JavaScript í´ë¦­
                            await page.evaluate('''
                                () => {
                                    const button = document.querySelector('div.SearchMore.upper');
                                    if (button) button.click();
                                }
                            ''')
                    
                    click_count += 1
                    self.logger.info(f"'ë§›ì§‘ ë”ë³´ê¸°' ë²„íŠ¼ í´ë¦­ ({click_count}íšŒ)")
                    
                    # ë¡œë”© ëŒ€ê¸° (ì ì§„ì  ì¦ê°€)
                    wait_time = min(3 + (click_count * 0.1), 5)  # ìµœëŒ€ 5ì´ˆ
                    await asyncio.sleep(wait_time)
                else:
                    # ë²„íŠ¼ì´ ë” ì´ìƒ ë³´ì´ì§€ ì•Šìœ¼ë©´ ì¢…ë£Œ
                    self.logger.info(f"'ë§›ì§‘ ë”ë³´ê¸°' ë²„íŠ¼ì´ ì‚¬ë¼ì¡ŒìŠµë‹ˆë‹¤. (ì´ {click_count}íšŒ í´ë¦­)")
                    break
                    
            except TimeoutError:
                # íƒ€ì„ì•„ì›ƒ = ë²„íŠ¼ì´ ë” ì´ìƒ ì—†ìŒ
                self.logger.info(f"'ë§›ì§‘ ë”ë³´ê¸°' ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì´ {click_count}íšŒ í´ë¦­)")
                break
            except Exception as e:
                self.logger.warning(f"ë²„íŠ¼ í´ë¦­ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
                break
        
        if click_count >= max_attempts:
            self.logger.warning(f"ìµœëŒ€ í´ë¦­ íšŸìˆ˜({max_attempts})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
    
    async def _extract_restaurants_from_page(self, page: Page) -> list:
        """
        í˜„ì¬ í˜ì´ì§€ì—ì„œ ìŒì‹ì  ì´ë¦„ ì¶”ì¶œ (ìˆ«ì. ì œê±°)
        
        Args:
            page: Playwright Page ê°ì²´
            
        Returns:
            list: [(ìŒì‹ì ëª…, ""), ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ (ì£¼ì†ŒëŠ” ë¹„ì–´ìˆìŒ)
        """
        restaurants = []
        
        try:
            # ğŸ”¥ ì—¬ëŸ¬ ì„ íƒì ì‹œë„
            selectors = [
                '[id^="title"]',  # IDê°€ titleë¡œ ì‹œì‘
                'div[id^="title"]',
                'span[id^="title"]',
                'a[id^="title"]'
            ]
            
            title_elements = []
            
            for selector in selectors:
                try:
                    elements = await page.locator(selector).all()
                    if elements:
                        title_elements = elements
                        self.logger.info(f"ì´ {len(title_elements)}ê°œ title ìš”ì†Œ ë°œê²¬ (ì„ íƒì: {selector})")
                        break
                except:
                    continue
            
            if not title_elements:
                self.logger.error("ìŒì‹ì  ì´ë¦„ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                # ğŸ”¥ ë””ë²„ê¹…: í˜ì´ì§€ HTML ì¼ë¶€ ì¶œë ¥
                try:
                    html_sample = await page.content()
                    self.logger.debug(f"í˜ì´ì§€ HTML ìƒ˜í”Œ (ì²« 1000ì): {html_sample[:1000]}")
                except:
                    pass
                
                return []
            
            for idx, element in enumerate(title_elements, 1):
                try:
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text = await element.inner_text(timeout=3000)
                    
                    if text and text.strip():
                        # ìˆ«ìì™€ ì (.) ì œê±° (ì˜ˆ: "1. ìŠ¤íƒ€ë²…ìŠ¤" â†’ "ìŠ¤íƒ€ë²…ìŠ¤")
                        # ì •ê·œì‹: ìˆ«ì + ì  + ê³µë°± ì œê±°
                        cleaned_name = re.sub(r'^\d+\.\s*', '', text.strip())
                        
                        if cleaned_name:
                            restaurants.append((cleaned_name, ""))  # ì£¼ì†ŒëŠ” ë¹ˆ ë¬¸ìì—´
                            
                            # ë¡œê¹… (10ê°œë§ˆë‹¤)
                            if idx % 10 == 0:
                                self.logger.debug(f"ì¶”ì¶œ ì¤‘... {idx}ê°œ")
                    
                except Exception as item_error:
                    self.logger.error(f"ì•„ì´í…œ {idx} ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {item_error}")
                    continue
            
            self.logger.info(f"ìŒì‹ì  ì´ë¦„ ì¶”ì¶œ ì™„ë£Œ: {len(restaurants)}ê°œ")
            
        except TimeoutError:
            self.logger.error("ìŒì‹ì  ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ìŒì‹ì  ëª©ë¡ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
        
        return restaurants
    
    async def _process_batch_parallel(
        self, 
        page: Page, 
        batch: list, 
        batch_start: int, 
        total: int, 
        delay: int
    ):
        """ë°°ì¹˜ ë³‘ë ¬ í¬ë¡¤ë§"""
        try:
            # ë³‘ë ¬ ì²˜ë¦¬: CrawlingManager ì‚¬ìš©
            crawling_manager = CrawlingManager("DiningCode")
            
            await crawling_manager.execute_crawling_with_save(
                stores=batch,
                crawl_func=lambda store, idx, t: self._crawl_single_store_parallel(page, store),
                save_func=self._save_wrapper_with_total(batch_start, total),
                delay=delay
            )
            
            # ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
            self.success_count += crawling_manager.success_count
            self.fail_count += crawling_manager.fail_count
            
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
    
    async def _crawl_single_store_parallel(self, page: Page, store: tuple):
        """
        ë‹¨ì¼ ë§¤ì¥ í¬ë¡¤ë§ (ë³‘ë ¬ìš©)
        
        Args:
            page: Playwright Page ê°ì²´
            store: (name, "") íŠœí”Œ
            
        Returns:
            Tuple: (store_data, name) ë˜ëŠ” None
        """
        name, _ = store  # ì£¼ì†ŒëŠ” ë¹„ì–´ìˆìŒ
        
        try:
            # ê²€ìƒ‰ ì „ëµ ì‚¬ìš© (ì´ë¦„ë§Œìœ¼ë¡œ ê²€ìƒ‰)
            async def extract_callback(entry_frame, page):
                extractor = StoreDetailExtractor(entry_frame, page)
                return await extractor.extract_all_details()
            
            # ì£¼ì†Œ ì—†ì´ ì´ë¦„ë§Œìœ¼ë¡œ ê²€ìƒ‰
            store_data = await self.search_strategy.search_with_multiple_strategies(
                page=page,
                store_name=name,
                road_address="",  # ì£¼ì†Œ ì—†ìŒ
                extractor_callback=extract_callback
            )
            
            if store_data:
                # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
                await OptimizedBrowserManager.clear_page_resources(page)
                return (store_data, name)
            
            return None
            
        except Exception as e:
            self.logger.error(f"'{name}' í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _save_wrapper_with_total(self, batch_start: int, total: int):
        """ì €ì¥ ë˜í¼ íŒ©í† ë¦¬"""
        async def wrapper(idx: int, total_stores: int, store_data_tuple, store_name: str):
            if store_data_tuple is None:
                return (False, "í¬ë¡¤ë§ ì‹¤íŒ¨")
            
            store_data, actual_name = store_data_tuple
            global_idx = batch_start + idx
            
            return await self.data_saver.save_store_data(
                idx=global_idx,
                total=total,
                store_data=store_data,
                store_name=actual_name,
                log_prefix="DiningCode"
            )
        
        return wrapper


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger = get_logger(__name__)
    
    logger.info("DiningCode ìŒì‹ì  í¬ë¡¤ëŸ¬ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)")
    
    try:
        crawler = DiningCodeRestaurantCrawler(headless=True)
        
        await crawler.crawl_all_pages(
            delay=5,
            naver_delay=15
        )
        
        logger.info("í¬ë¡¤ëŸ¬ ì¢…ë£Œ")
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())


if __name__ == "__main__":
    asyncio.run(main())