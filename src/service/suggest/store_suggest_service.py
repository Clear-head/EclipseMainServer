"""
ê°œì„ ëœ ë§¤ì¥ ì œì•ˆ ì„œë¹„ìŠ¤ (í‚¤ì›Œë“œ ë§¤ì¹­ + ì‹œë§¨í‹± ê²€ìƒ‰ í•˜ì´ë¸Œë¦¬ë“œ)
"""
from typing import List, Dict, Optional
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer, CrossEncoder
import torch
import re

from src.infra.external.query_enchantment import QueryEnhancementService
from src.logger.custom_logger import get_logger

logger = get_logger(__name__)


class StoreSuggestService:
    """ê°œì„ ëœ ë§¤ì¥ ì œì•ˆ ì„œë¹„ìŠ¤ (í‚¤ì›Œë“œ ë§¤ì¹­ ì¤‘ì‹¬)"""
    
    def __init__(self, persist_directory: str = "./chroma_db", use_reranker: bool = True):
        """
        Args:
            persist_directory: ChromaDB ì €ì¥ ê²½ë¡œ
            use_reranker: Re-ranking ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€
        """
        logger.info("ê°œì„ ëœ ë§¤ì¥ ì œì•ˆ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: intfloat/multilingual-e5-large")
        self.embedding_model = SentenceTransformer(
            "intfloat/multilingual-e5-large",
            device=self.device
        )
        logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # Re-ranking ëª¨ë¸ ë¡œë“œ (í•œêµ­ì–´ íŠ¹í™”)
        self.use_reranker = use_reranker
        self.reranker = None
        
        if self.use_reranker:
            try:
                logger.info("Re-ranking ëª¨ë¸ ë¡œë”© ì¤‘: BAAI/bge-reranker-base")
                self.reranker = CrossEncoder(
                    'BAAI/bge-reranker-base',
                    max_length=512,
                    device=self.device
                )
                logger.info(f"Re-ranking ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                logger.error(f"Re-ranking ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.use_reranker = False
        
        self.query_enhancer = QueryEnhancementService()
        
        try:
            self.store_collection = self.client.get_collection(name="stores")
            logger.info(f"ë§¤ì¥ ì»¬ë ‰ì…˜ ë¡œë“œ ì™„ë£Œ: {self.store_collection.count()}ê°œ ë§¤ì¥")
        except Exception as e:
            logger.error(f"ë§¤ì¥ ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            raise
    
    @staticmethod
    def convert_type_to_code(type_korean: str) -> str:
        """í•œê¸€ íƒ€ì…ì„ ì½”ë“œë¡œ ë³€í™˜"""
        type_map = {"ìŒì‹ì ": "0", "ì¹´í˜": "1", "ì½˜í…ì¸ ": "2"}
        return type_map.get(type_korean, "")
    
    def extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‰¼í‘œ, ê³µë°± ê¸°ì¤€)"""
        # ì‰¼í‘œì™€ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        keywords = re.split(r'[,\s]+', text)
        # ë¹ˆ ë¬¸ìì—´ ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
        keywords = [k.strip() for k in keywords if k.strip()]
        return keywords
    
    def calculate_keyword_score(self, query_keywords: List[str], document: str) -> float:
        """
        í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (BM25 ìŠ¤íƒ€ì¼)
        
        Args:
            query_keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            document: ë¬¸ì„œ í…ìŠ¤íŠ¸
            
        Returns:
            float: í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (0~1)
        """
        if not query_keywords:
            return 0.0
        
        doc_lower = document.lower()
        
        # ê° í‚¤ì›Œë“œê°€ ë¬¸ì„œì— ë“±ì¥í•˜ëŠ”ì§€ í™•ì¸
        matches = 0
        total_occurrences = 0
        
        for keyword in query_keywords:
            keyword_lower = keyword.lower()
            count = doc_lower.count(keyword_lower)
            if count > 0:
                matches += 1
                total_occurrences += count
        
        # ë§¤ì¹­ ë¹„ìœ¨ ê³„ì‚°
        match_ratio = matches / len(query_keywords)
        
        # ë¹ˆë„ ì ìˆ˜ (ë¡œê·¸ ìŠ¤ì¼€ì¼)
        import math
        frequency_score = math.log1p(total_occurrences) / 5.0  # ì •ê·œí™”
        
        # ìµœì¢… ì ìˆ˜: ë§¤ì¹­ ë¹„ìœ¨ 70% + ë¹ˆë„ 30%
        final_score = (match_ratio * 0.7) + (min(frequency_score, 1.0) * 0.3)
        
        return final_score
    
    def hybrid_rerank(
        self,
        search_query: str,
        query_keywords: List[str],
        ids: List[str],
        metadatas: List[Dict],
        documents: List[str],
        distances: List[float],
        keyword_weight: float = 0.5,
        semantic_weight: float = 0.3,
        rerank_weight: float = 0.2
    ) -> List[tuple]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ Re-ranking: í‚¤ì›Œë“œ + ì‹œë§¨í‹± + Cross-Encoder
        
        Args:
            search_query: ê²€ìƒ‰ ì¿¼ë¦¬
            query_keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            ids: ë§¤ì¥ ID ë¦¬ìŠ¤íŠ¸
            metadatas: ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            distances: ê±°ë¦¬ ë¦¬ìŠ¤íŠ¸
            keyword_weight: í‚¤ì›Œë“œ ë§¤ì¹­ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 50%)
            semantic_weight: ì‹œë§¨í‹± ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 30%)
            rerank_weight: Re-ranker ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 20%)
            
        Returns:
            List[tuple]: (id, metadata, document, final_score, score_details) í˜•íƒœ
        """
        logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ Re-ranking ì‹œì‘: {len(ids)}ê°œ ë¬¸ì„œ")
        logger.info(f"ê°€ì¤‘ì¹˜ - í‚¤ì›Œë“œ:{keyword_weight}, ì‹œë§¨í‹±:{semantic_weight}, Re-rank:{rerank_weight}")
        
        results = []
        
        # Cross-Encoder ì ìˆ˜ ê³„ì‚° (ì‚¬ìš©í•˜ëŠ” ê²½ìš°)
        rerank_scores = None
        if self.use_reranker and self.reranker is not None:
            try:
                pairs = [[search_query, doc] for doc in documents]
                rerank_scores = self.reranker.predict(pairs)
                logger.info("Cross-Encoder ì ìˆ˜ ê³„ì‚° ì™„ë£Œ")
            except Exception as e:
                logger.error(f"Cross-Encoder ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                rerank_scores = None
        
        # ê° ë¬¸ì„œì— ëŒ€í•´ ì ìˆ˜ ê³„ì‚°
        for i in range(len(ids)):
            # 1. í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            keyword_score = self.calculate_keyword_score(query_keywords, documents[i])
            
            # 2. ì‹œë§¨í‹± ìœ ì‚¬ë„ ì ìˆ˜ (ê±°ë¦¬ -> ìœ ì‚¬ë„)
            semantic_score = max(0, 1 - distances[i])
            
            # 3. Re-ranker ì ìˆ˜ (ì •ê·œí™”: -10~10 -> 0~1)
            if rerank_scores is not None:
                rerank_score = (rerank_scores[i] + 10) / 20  # ì •ê·œí™”
                rerank_score = max(0, min(1, rerank_score))  # í´ë¦¬í•‘
            else:
                rerank_score = semantic_score  # Re-ranker ì—†ìœ¼ë©´ ì‹œë§¨í‹± ì ìˆ˜ ì‚¬ìš©
            
            # 4. ìµœì¢… ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
            final_score = (
                keyword_score * keyword_weight +
                semantic_score * semantic_weight +
                rerank_score * rerank_weight
            )
            
            score_details = {
                'keyword': round(keyword_score, 4),
                'semantic': round(semantic_score, 4),
                'rerank': round(rerank_score, 4),
                'final': round(final_score, 4)
            }
            
            results.append((
                ids[i],
                metadatas[i],
                documents[i],
                final_score,
                score_details
            ))
        
        # ìµœì¢… ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda x: x[3], reverse=True)
        
        logger.info("í•˜ì´ë¸Œë¦¬ë“œ Re-ranking ì™„ë£Œ")
        logger.info(f"ìƒìœ„ 3ê°œ ì ìˆ˜: {[r[4] for r in results[:3]]}")
        
        return results
    
    async def suggest_stores(
        self,
        personnel: Optional[int] = None,
        region: Optional[str] = None,
        category_type: Optional[str] = None,
        user_keyword: str = "",
        n_results: int = 20,
        use_ai_enhancement: bool = False,
        min_similarity_threshold: float = 0.2,
        rerank_candidates_multiplier: int = 5,
        keyword_weight: float = 0.75,
        semantic_weight: float = 0.2,
        rerank_weight: float = 0.1
    ) -> List[Dict]:
        """ê°œì„ ëœ ë§¤ì¥ ì œì•ˆ (í‚¤ì›Œë“œ ì¤‘ì‹¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)"""
        
        logger.info("=" * 60)
        logger.info("ê°œì„ ëœ ë§¤ì¥ ì œì•ˆ ìš”ì²­")
        logger.info(f"  - ì¸ì›: {personnel}ëª…")
        logger.info(f"  - ì§€ì—­: {region}")
        logger.info(f"  - íƒ€ì…: {category_type}")
        logger.info(f"  - ì›ë³¸ í‚¤ì›Œë“œ: {user_keyword}")
        logger.info("=" * 60)
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        query_keywords = self.extract_keywords(user_keyword)
        logger.info(f"ì¶”ì¶œëœ í‚¤ì›Œë“œ: {query_keywords}")
        
        # ğŸ”¥ í‚¤ì›Œë“œ ì „ì²˜ë¦¬ (ë™ì˜ì–´ ì¹˜í™˜)
        query_keywords = self.preprocess_keywords(query_keywords)
        logger.info(f"ì „ì²˜ë¦¬ëœ í‚¤ì›Œë“œ: {query_keywords}")
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        if use_ai_enhancement:
            # AI ì¿¼ë¦¬ ê°œì„  ì‚¬ìš© ì‹œ
            search_query = await self.query_enhancer.enhance_query(
                personnel=personnel,
                category_type=category_type,
                user_keyword=user_keyword  # ì›ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©
            )
        else:
            # ğŸ”¥ ì¹˜í™˜ëœ í‚¤ì›Œë“œë¡œ ì¿¼ë¦¬ ìƒì„±
            query_parts = []
            if category_type:
                query_parts.append(category_type)
            query_parts.extend(query_keywords)  # ì¹˜í™˜ëœ í‚¤ì›Œë“œ ì‚¬ìš©
            search_query = " ".join(query_parts) if query_parts else user_keyword
        
        logger.info(f"ìµœì¢… ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")
        
        # ë©”íƒ€ë°ì´í„° í•„í„°
        where_filter = None
        filter_conditions = []
        
        if region:
            filter_conditions.append({"region": region})
        
        if category_type:
            type_code = self.convert_type_to_code(category_type)
            if type_code:
                filter_conditions.append({"type_code": type_code})
        
        if len(filter_conditions) > 1:
            where_filter = {"$and": filter_conditions}
        elif len(filter_conditions) == 1:
            where_filter = filter_conditions[0]
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedding_model.encode(
            search_query,
            convert_to_tensor=True,
            show_progress_bar=False
        )
        
        if self.device == "cuda":
            query_embedding = query_embedding.cpu()
        
        # ChromaDB ê²€ìƒ‰ (ğŸ”¥ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        search_n_results = n_results * rerank_candidates_multiplier
        
        try:
            # ğŸ”¥ include íŒŒë¼ë¯¸í„°ì—ì„œ 'embeddings' ì œê±° (ID ì˜¤ë¥˜ ë°©ì§€)
            results = self.store_collection.query(
                query_embeddings=[query_embedding.numpy().tolist()],
                n_results=search_n_results,
                where=where_filter,
                include=["metadatas", "documents", "distances"]  # embeddings ì œì™¸
            )
            
            logger.info(f"ChromaDB ê²€ìƒ‰ ê²°ê³¼: {len(results['ids'][0])}ê°œ")
            
        except Exception as e:
            logger.error(f"ChromaDB ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
        
        if not results['ids'][0]:
            logger.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # í•˜ì´ë¸Œë¦¬ë“œ Re-ranking
        reranked_results = self.hybrid_rerank(
            search_query=search_query,
            query_keywords=query_keywords,
            ids=results['ids'][0],
            metadatas=results['metadatas'][0],
            documents=results['documents'][0],
            distances=results['distances'][0],
            keyword_weight=keyword_weight,
            semantic_weight=semantic_weight,
            rerank_weight=rerank_weight
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        suggestions = []
        
        for store_id, metadata, document, final_score, score_details in reranked_results:
            try:
                if final_score < min_similarity_threshold:
                    continue
                
                suggestion = {
                    'store_id': metadata.get('store_id'),
                    'region': metadata.get('region'),
                    'type': metadata.get('type'),
                    'business_hour': metadata.get('business_hour'),
                    'similarity_score': final_score,
                    'score_breakdown': score_details,
                    'document': document,
                    'search_query': search_query
                }
                
                suggestions.append(suggestion)
                
                if len(suggestions) >= n_results:
                    break
                
            except Exception as e:
                logger.error(f"ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ìµœì¢… ì œì•ˆ ê²°ê³¼: {len(suggestions)}ê°œ")
        
        # ìƒìœ„ 3ê°œ ê²°ê³¼ ë¡œê¹…
        for i, sug in enumerate(suggestions[:3], 1):
            logger.info(f"ìˆœìœ„ {i}: ìµœì¢…ì ìˆ˜={sug['similarity_score']:.4f}, ì„¸ë¶€={sug['score_breakdown']}")
        
        return suggestions
    
    async def get_store_details(self, store_ids: List[str]) -> List[Dict]:
        """ë§¤ì¥ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        from src.infra.database.repository.category_repository import CategoryRepository
        
        category_repo = CategoryRepository()
        store_details = []
        
        for store_id in store_ids:
            try:
                stores = await category_repo.select(id=store_id)
                if stores and len(stores) > 0:
                    store = stores[0]
                    store_dict = {
                        'id': store.id,
                        'name': store.name,
                        'do': store.do,
                        'si': store.si,
                        'gu': store.gu,
                        'detail_address': store.detail_address,
                        'sub_category': store.sub_category,
                        'business_hour': store.business_hour,
                        'phone': store.phone,
                        'type': store.type,
                        'image': store.image,
                        'latitude': store.latitude,
                        'longitude': store.longitude,
                        'menu': store.menu
                    }
                    store_details.append(store_dict)
            except Exception as e:
                logger.error(f"ë§¤ì¥ ID '{store_id}' ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        return store_details
    
    def preprocess_keywords(self, keywords: List[str]) -> List[str]:
        """
        í‚¤ì›Œë“œ ì „ì²˜ë¦¬ (ë™ì˜ì–´ ì¹˜í™˜)
        
        Args:
            keywords: ì›ë³¸ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[str]: ì¹˜í™˜ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        # ë™ì˜ì–´ ë§¤í•‘
        synonym_map = {
            "ì¤‘êµ­ì§‘": "ì¤‘ì‹ë‹¹",
            "ì¤‘êµ­ìš”ë¦¬": "ì¤‘ì‹ë‹¹",
            "ì¤‘êµ­ìŒì‹": "ì¤‘ì‹ë‹¹",
            "í•œì‹ì§‘": "í•œì‹",
            # í•„ìš”í•œ ë§Œí¼ ì¶”ê°€
        }
        
        processed_keywords = []
        for keyword in keywords:
            # ë™ì˜ì–´ê°€ ìˆìœ¼ë©´ ì¹˜í™˜, ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
            processed = synonym_map.get(keyword.strip(), keyword.strip())
            processed_keywords.append(processed)
            
            if processed != keyword.strip():
                logger.info(f"í‚¤ì›Œë“œ ì¹˜í™˜: '{keyword}' â†’ '{processed}'")
        
        return processed_keywords